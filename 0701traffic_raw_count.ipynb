{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read and filter the original files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_103_filtered.csv\n",
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_107_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_8464\\3120397016.py:10: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_109_filtered.csv\n",
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_110_filtered.csv\n",
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_145_filtered.csv\n",
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_174_filtered.csv\n",
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_96_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Read and filter the original files\n",
    "dfl_file_path = 'Data/dfl_traffic_count'\n",
    "years_to_filter = [2018, 2019, 2021, 2022, 2023, 2024]\n",
    "\n",
    "# List all files in the directory\n",
    "file_paths = [os.path.join(dfl_file_path, file) for file in os.listdir(dfl_file_path) if file.endswith('.csv')]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if 'count_date' column exists\n",
    "    if 'count_date' not in df.columns:\n",
    "        print(f\"'count_date' column not found in {file_path}. Skipping this file.\")\n",
    "        continue\n",
    "    \n",
    "    # Convert 'count_date' column to datetime type\n",
    "    df['count_date'] = pd.to_datetime(df['count_date'], errors='coerce')\n",
    "    \n",
    "    # Filter rows for the specified years\n",
    "    filtered_df = df[df['count_date'].dt.year.isin(years_to_filter)]\n",
    "    \n",
    "    # Construct new file name for the filtered data\n",
    "    new_file_path = file_path.replace('.csv', '_filtered.csv')\n",
    "    \n",
    "    # Save the filtered data to a new file\n",
    "    filtered_df.to_csv(new_file_path, index=False)\n",
    "    print(f\"Filtered data saved to {new_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Processing filtered files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_8464\\3383438608.py:15: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  daily_df = df.groupby('date').mean().reset_index()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid fill method. Expecting pad (ffill) or backfill (bfill). Got spline",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mn:\\MSc_grad_proj\\0701traffic_raw_count.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m daily_df\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m}, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Interpolate missing values using spline interpolation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m daily_df\u001b[39m.\u001b[39;49minterpolate(method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mspline\u001b[39;49m\u001b[39m'\u001b[39;49m, order\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Save the processed data to a new CSV file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m processed_file_path \u001b[39m=\u001b[39m file_path\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_processed.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:11855\u001b[0m, in \u001b[0;36mDataFrame.interpolate\u001b[1;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[0;32m  11843\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m  11844\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minterpolate\u001b[39m(\n\u001b[0;32m  11845\u001b[0m     \u001b[39mself\u001b[39m: DataFrame,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11853\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m  11854\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m> 11855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39minterpolate(\n\u001b[0;32m  11856\u001b[0m         method,\n\u001b[0;32m  11857\u001b[0m         axis,\n\u001b[0;32m  11858\u001b[0m         limit,\n\u001b[0;32m  11859\u001b[0m         inplace,\n\u001b[0;32m  11860\u001b[0m         limit_direction,\n\u001b[0;32m  11861\u001b[0m         limit_area,\n\u001b[0;32m  11862\u001b[0m         downcast,\n\u001b[0;32m  11863\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m  11864\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:7568\u001b[0m, in \u001b[0;36mNDFrame.interpolate\u001b[1;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[0;32m   7562\u001b[0m \u001b[39mif\u001b[39;00m isna(index)\u001b[39m.\u001b[39many():\n\u001b[0;32m   7563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   7564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInterpolation with NaNs in the index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   7565\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhas not been implemented. Try filling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   7566\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthose NaNs before interpolating.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   7567\u001b[0m     )\n\u001b[1;32m-> 7568\u001b[0m new_data \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39minterpolate(\n\u001b[0;32m   7569\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m   7570\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m   7571\u001b[0m     index\u001b[39m=\u001b[39mindex,\n\u001b[0;32m   7572\u001b[0m     limit\u001b[39m=\u001b[39mlimit,\n\u001b[0;32m   7573\u001b[0m     limit_direction\u001b[39m=\u001b[39mlimit_direction,\n\u001b[0;32m   7574\u001b[0m     limit_area\u001b[39m=\u001b[39mlimit_area,\n\u001b[0;32m   7575\u001b[0m     inplace\u001b[39m=\u001b[39minplace,\n\u001b[0;32m   7576\u001b[0m     downcast\u001b[39m=\u001b[39mdowncast,\n\u001b[0;32m   7577\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   7578\u001b[0m )\n\u001b[0;32m   7580\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\n\u001b[0;32m   7581\u001b[0m \u001b[39mif\u001b[39;00m should_transpose:\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:422\u001b[0m, in \u001b[0;36mBaseBlockManager.interpolate\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minterpolate\u001b[39m(\u001b[39mself\u001b[39m: T, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m--> 422\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply(\u001b[39m\"\u001b[39m\u001b[39minterpolate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(b, f)(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1619\u001b[0m, in \u001b[0;36mEABackedBlock.interpolate\u001b[1;34m(self, method, axis, inplace, limit, fill_value, **kwargs)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     new_values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mfillna(value\u001b[39m=\u001b[39mfill_value, method\u001b[39m=\u001b[39mmethod, limit\u001b[39m=\u001b[39mlimit)\u001b[39m.\u001b[39mT\n\u001b[0;32m   1618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1619\u001b[0m     new_values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49mfillna(value\u001b[39m=\u001b[39;49mfill_value, method\u001b[39m=\u001b[39;49mmethod, limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[0;32m   1620\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block_same_class(new_values)\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:317\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.fillna\u001b[1;34m(self, value, method, limit)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39m@doc\u001b[39m(ExtensionArray\u001b[39m.\u001b[39mfillna)\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfillna\u001b[39m(\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m: NDArrayBackedExtensionArrayT, value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, method\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    316\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDArrayBackedExtensionArrayT:\n\u001b[1;32m--> 317\u001b[0m     value, method \u001b[39m=\u001b[39m validate_fillna_kwargs(\n\u001b[0;32m    318\u001b[0m         value, method, validate_scalar_dict_value\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    319\u001b[0m     )\n\u001b[0;32m    321\u001b[0m     mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misna()\n\u001b[0;32m    322\u001b[0m     \u001b[39m# error: Argument 2 to \"check_value_size\" has incompatible type\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[39m# \"ExtensionArray\"; expected \"ndarray\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\util\\_validators.py:390\u001b[0m, in \u001b[0;36mvalidate_fillna_kwargs\u001b[1;34m(value, method, validate_scalar_dict_value)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMust specify a fill \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    389\u001b[0m \u001b[39melif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m     method \u001b[39m=\u001b[39m clean_fill_method(method)\n\u001b[0;32m    392\u001b[0m \u001b[39melif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m     \u001b[39mif\u001b[39;00m validate_scalar_dict_value \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(value, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\missing.py:125\u001b[0m, in \u001b[0;36mclean_fill_method\u001b[1;34m(method, allow_nearest)\u001b[0m\n\u001b[0;32m    123\u001b[0m     expecting \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpad (ffill), backfill (bfill) or nearest\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m valid_methods:\n\u001b[1;32m--> 125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid fill method. Expecting \u001b[39m\u001b[39m{\u001b[39;00mexpecting\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mmethod\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m method\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid fill method. Expecting pad (ffill) or backfill (bfill). Got spline"
     ]
    }
   ],
   "source": [
    "# Step 2: Processing filtered files\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Construct the filtered file path\n",
    "    filtered_file_path = file_path.replace('.csv', '_filtered.csv')\n",
    "    \n",
    "    # Read the filtered CSV file\n",
    "    df = pd.read_csv(filtered_file_path)\n",
    "    \n",
    "    # Convert 'count_date' to datetime and extract the date part\n",
    "    df['count_date'] = pd.to_datetime(df['count_date'])\n",
    "    df['date'] = df['count_date'].dt.date\n",
    "    \n",
    "    # Group by day and calculate the mean\n",
    "    daily_df = df.groupby('date').mean().reset_index()\n",
    "    \n",
    "    # Create a complete date range\n",
    "    full_date_range = pd.date_range(start=daily_df['date'].min(), end=daily_df['date'].max())\n",
    "    \n",
    "    # Reindex to have a continuous date range\n",
    "    daily_df = daily_df.set_index('date').reindex(full_date_range).reset_index()\n",
    "    daily_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "    \n",
    "    # Interpolate missing values using spline interpolation\n",
    "    daily_df.interpolate(method='spline', order=3, inplace=True)\n",
    "    \n",
    "    # Save the processed data to a new CSV file\n",
    "    processed_file_path = file_path.replace('.csv', '_processed.csv')\n",
    "    daily_df.to_csv(processed_file_path, index=False)\n",
    "    \n",
    "    print(f\"Processed data saved to {processed_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_103_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_8464\\2640638767.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['date'] = filtered_df['count_date'].dt.date\n",
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_8464\\2640638767.py:21: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  daily_df = filtered_df.groupby('date').mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_103_daily.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'count_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'count_date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mn:\\MSc_grad_proj\\0701traffic_raw_count.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Process each file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m file_path \u001b[39min\u001b[39;00m file_paths:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     process_file(file_path)\n",
      "\u001b[1;32mn:\\MSc_grad_proj\\0701traffic_raw_count.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_path, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mISO-8859-1\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Use ISO-8859-1 encoding to handle special characters\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Convert 'count_date' to datetime type\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mcount_date\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[\u001b[39m'\u001b[39;49m\u001b[39mcount_date\u001b[39;49m\u001b[39m'\u001b[39;49m], dayfirst\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoerce\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Filter rows based on the specified years\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/0701traffic_raw_count.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m filtered_df \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mcount_date\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39myear\u001b[39m.\u001b[39misin(years_to_filter)]\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'count_date'"
     ]
    }
   ],
   "source": [
    "# Function to process each file\n",
    "def process_file(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')  # Use ISO-8859-1 encoding to handle special characters\n",
    "    \n",
    "    # Convert 'count_date' to datetime type\n",
    "    df['count_date'] = pd.to_datetime(df['count_date'], dayfirst=True, errors='coerce')\n",
    "    \n",
    "    # Filter rows based on the specified years\n",
    "    filtered_df = df[df['count_date'].dt.year.isin(years_to_filter)]\n",
    "    \n",
    "    # Save the filtered data to a new file\n",
    "    new_file_path = file_path.replace('.csv', '_filtered.csv')\n",
    "    filtered_df.to_csv(new_file_path, index=False)\n",
    "    print(f\"Filtered data saved to {new_file_path}\")\n",
    "    \n",
    "    # Convert 'count_date' to datetime and extract date part\n",
    "    filtered_df['date'] = filtered_df['count_date'].dt.date\n",
    "    \n",
    "    # Group by day and calculate the mean\n",
    "    daily_df = filtered_df.groupby('date').mean().reset_index()\n",
    "    \n",
    "    # Create a complete date range\n",
    "    full_date_range = pd.date_range(start=daily_df['date'].min(), end=daily_df['date'].max())\n",
    "    \n",
    "    # Reindex to have a continuous date range\n",
    "    daily_df = daily_df.set_index('date').reindex(full_date_range).reset_index()\n",
    "    daily_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "    \n",
    "    # Interpolate missing values for numerical columns\n",
    "    numeric_cols = daily_df.select_dtypes(include='number').columns\n",
    "    daily_df[numeric_cols] = daily_df[numeric_cols].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    # Save the processed data to a new CSV file\n",
    "    output_file_path = new_file_path.replace('_filtered.csv', '_daily.csv')\n",
    "    daily_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Processed data saved to {output_file_path}\")\n",
    "\n",
    "# Get all CSV files in the input folder\n",
    "file_paths = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Process each file\n",
    "for file_path in file_paths:\n",
    "    process_file(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl_file_path = 'Data\\dfl_traffic_count'\n",
    "\n",
    "dft93 = 'Data\\\\traffic_count\\dft_rawcount_local_authority_id_93.csv'\n",
    "dft96 = 'Data\\\\traffic_count\\dft_rawcount_local_authority_id_96.csv'\n",
    "dft109 = 'Data\\\\traffic_count\\dft_rawcount_local_authority_id_109.csv'\n",
    "dft110 = 'Data\\\\traffic_count\\dft_rawcount_local_authority_id_110.csv'\n",
    "dft145 = 'Data\\\\traffic_count\\dft_rawcount_local_authority_id_145.csv'\n",
    "dft174 = 'Data\\\\traffic_count\\dft_rawcount_local_authority_id_174.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to Data\\traffic_count\\dft_rawcount_local_authority_id_93_filtered.csv\n",
      "Filtered data saved to Data\\traffic_count\\dft_rawcount_local_authority_id_96_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_27736\\327191506.py:18: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to Data\\traffic_count\\dft_rawcount_local_authority_id_109_filtered.csv\n",
      "Filtered data saved to Data\\traffic_count\\dft_rawcount_local_authority_id_110_filtered.csv\n",
      "Filtered data saved to Data\\traffic_count\\dft_rawcount_local_authority_id_145_filtered.csv\n",
      "Filtered data saved to Data\\traffic_count\\dft_rawcount_local_authority_id_174_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 定义文件路径列表\n",
    "file_paths = [\n",
    "    'Data\\\\traffic_count\\\\dft_rawcount_local_authority_id_93.csv',\n",
    "    'Data\\\\traffic_count\\\\dft_rawcount_local_authority_id_96.csv',\n",
    "    'Data\\\\traffic_count\\\\dft_rawcount_local_authority_id_109.csv',\n",
    "    'Data\\\\traffic_count\\\\dft_rawcount_local_authority_id_110.csv',\n",
    "    'Data\\\\traffic_count\\\\dft_rawcount_local_authority_id_145.csv',\n",
    "    'Data\\\\traffic_count\\\\dft_rawcount_local_authority_id_174.csv'\n",
    "]\n",
    "\n",
    "# 指定需要过滤的年份\n",
    "years_to_filter = [2018, 2019, 2021, 2022, 2023, 2024]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 将count_date列转换为datetime类型\n",
    "    df['count_date'] = pd.to_datetime(df['count_date'])\n",
    "    \n",
    "    # 过滤出指定年份的行\n",
    "    filtered_df = df[df['count_date'].dt.year.isin(years_to_filter)]\n",
    "    \n",
    "    # 构建新的文件名\n",
    "    new_file_path = file_path.replace('.csv', '_filtered.csv')\n",
    "    \n",
    "    # 保存过滤后的数据到新文件\n",
    "    filtered_df.to_csv(new_file_path, index=False)\n",
    "\n",
    "    print(f\"Filtered data saved to {new_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
