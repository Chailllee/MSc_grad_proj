{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import UnivariateSpline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read and filter the original files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_103_filtered.csv\n",
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_107_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_8464\\3120397016.py:10: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_109_filtered.csv\n",
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_110_filtered.csv\n",
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_145_filtered.csv\n",
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_174_filtered.csv\n",
      "Filtered data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_96_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Read and filter the original files\n",
    "dfl_file_path = 'Data/dfl_traffic_count'\n",
    "years_to_filter = [2018, 2019, 2021, 2022, 2023, 2024]\n",
    "\n",
    "# List all files in the directory\n",
    "file_paths = [os.path.join(dfl_file_path, file) for file in os.listdir(dfl_file_path) if file.endswith('.csv')]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if 'count_date' column exists\n",
    "    if 'count_date' not in df.columns:\n",
    "        print(f\"'count_date' column not found in {file_path}. Skipping this file.\")\n",
    "        continue\n",
    "    \n",
    "    # Convert 'count_date' column to datetime type\n",
    "    df['count_date'] = pd.to_datetime(df['count_date'], errors='coerce')\n",
    "    \n",
    "    # Filter rows for the specified years\n",
    "    filtered_df = df[df['count_date'].dt.year.isin(years_to_filter)]\n",
    "    \n",
    "    # Construct new file name for the filtered data\n",
    "    new_file_path = file_path.replace('.csv', '_filtered.csv')\n",
    "    \n",
    "    # Save the filtered data to a new file\n",
    "    filtered_df.to_csv(new_file_path, index=False)\n",
    "    print(f\"Filtered data saved to {new_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Processing filtered files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_103_processed.csv\n",
      "Processed data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_107_processed.csv\n",
      "Processed data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_109_processed.csv\n",
      "Processed data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_110_processed.csv\n",
      "Processed data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_145_processed.csv\n",
      "Processed data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_174_processed.csv\n",
      "Processed data saved to Data/dfl_traffic_count\\dft_rawcount_local_authority_id_96_processed.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Process the filtered files\n",
    "filtered_file_paths = [file.replace('.csv', '_filtered.csv') for file in file_paths]\n",
    "\n",
    "for filtered_file_path in filtered_file_paths:\n",
    "    # Check if filtered file exists\n",
    "    if not os.path.exists(filtered_file_path):\n",
    "        print(f\"Filtered file {filtered_file_path} does not exist. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Read the filtered CSV file\n",
    "    df = pd.read_csv(filtered_file_path, encoding='ISO-8859-1')  # Adjust encoding if necessary\n",
    "    \n",
    "    # Convert 'count_date' to datetime and extract the date part\n",
    "    df['count_date'] = pd.to_datetime(df['count_date'], errors='coerce')\n",
    "    df['date'] = df['count_date'].dt.date\n",
    "    \n",
    "    # Group by day and calculate the mean for specific columns\n",
    "    columns_to_interpolate = [\n",
    "        'pedal_cycles', 'two_wheeled_motor_vehicles', 'cars_and_taxis', \n",
    "        'buses_and_coaches', 'lgvs', 'hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', \n",
    "        'hgvs_4_or_more_rigid_axle', 'hgvs_3_or_4_articulated_axle', \n",
    "        'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle', 'all_hgvs', \n",
    "        'all_motor_vehicles'\n",
    "    ]\n",
    "    daily_df = df.groupby('date')[columns_to_interpolate].mean().reset_index()\n",
    "    \n",
    "    # Create a complete date range\n",
    "    full_date_range = pd.date_range(start=daily_df['date'].min(), end=daily_df['date'].max())\n",
    "    \n",
    "    # Reindex to have a continuous date range\n",
    "    daily_df = daily_df.set_index('date').reindex(full_date_range).reset_index()\n",
    "    daily_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "    \n",
    "    # Interpolate missing values using spline interpolation for specified columns\n",
    "    for column in columns_to_interpolate:\n",
    "        mask = np.isfinite(daily_df[column])\n",
    "        x = np.arange(len(daily_df))\n",
    "        spline = UnivariateSpline(x[mask], daily_df.loc[mask, column], s=0)\n",
    "        daily_df[column] = spline(x)\n",
    "        daily_df[column] = daily_df[column].round(2)  # Round to 2 decimal places\n",
    "    \n",
    "    # Save the processed data to a new CSV file\n",
    "    processed_file_path = filtered_file_path.replace('_filtered.csv', '_processed.csv')\n",
    "    daily_df.to_csv(processed_file_path, index=False)\n",
    "    \n",
    "    print(f\"Processed data saved to {processed_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
