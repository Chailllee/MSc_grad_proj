{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AirQuality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义文件夹路径\n",
    "input_folder = r\"D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon\"\n",
    "output_folder = r\"D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\"\n",
    "\n",
    "# 确保输出文件夹存在\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义研究区域文件夹\n",
    "study_areas = [\"Camden\", \"City of London\", \"Islington\", \"Kensington and Chelsea\", \"Lambeth\", \"Southwark\", \"Westminster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Unify TimeScale of each.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process each study area\n",
    "for area in study_areas:\n",
    "    area_input_folder = os.path.join(input_folder, area)\n",
    "    area_output_folder = os.path.join(output_folder, area)\n",
    "    os.makedirs(area_output_folder, exist_ok=True)\n",
    "\n",
    "    # Process each CSV file in the study area folder\n",
    "    for filename in os.listdir(area_input_folder):\n",
    "        if \"NO2\" in filename or \"PM2.5\" in filename:\n",
    "            file_path = os.path.join(area_input_folder, filename)\n",
    "            \n",
    "            # Read the CSV file with encoding handling\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, encoding='latin1')\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read {file_path} with error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Check and rename columns if necessary\n",
    "            if 'Category' not in df.columns:\n",
    "                print(f\"'Category' column not found in {file_path}. Available columns: {df.columns}\")\n",
    "                continue\n",
    "            \n",
    "            # Clean data by dropping NaN values\n",
    "            df = df.dropna()\n",
    "            \n",
    "            # Extract the date from 'Category' column\n",
    "            df['Date'] = pd.to_datetime(df['Category'], dayfirst=True).dt.date\n",
    "            \n",
    "            # Determine the air quality type (NO2 or PM2.5)\n",
    "            if \"NO2\" in filename:\n",
    "                pollutant_type = \"NO2\"\n",
    "                pollutant_column = \"Nitrogen dioxide\"\n",
    "            elif \"PM2.5\" in filename:\n",
    "                pollutant_type = \"PM2.5\"\n",
    "                pollutant_column = \"PM<sub>2.5</sub> particulates\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Calculate daily max, mean, and min values\n",
    "            daily_stats = df.groupby('Date')[pollutant_column].agg(['mean', 'max', 'min']).reset_index()\n",
    "            daily_stats.columns = ['Date', f'{pollutant_type}_mean', f'{pollutant_type}_max', f'{pollutant_type}_min']\n",
    "            \n",
    "            # Generate the new file name\n",
    "            new_filename = filename.replace(\"Breathe London - \", \"\").replace(f\" - {pollutant_type}.csv\", f\"_{pollutant_type}.csv\")\n",
    "            new_file_path = os.path.join(area_output_folder, new_filename)\n",
    "            \n",
    "            # Save the new CSV file\n",
    "            daily_stats.to_csv(new_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "            print(f\"Processed and saved: {new_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kensington and Chelsea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_and_process_aqe_data(base_folder, region, save_folder):\n",
    "    coords_path = os.path.join(base_folder, 'coords_aqe.csv')\n",
    "    \n",
    "    try:\n",
    "        coords_df = pd.read_csv(coords_path)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reading {coords_path}: {e}\")\n",
    "        return\n",
    "    \n",
    "    pm25_data = []\n",
    "    pm10_data = []\n",
    "    \n",
    "    region_folder = os.path.join(base_folder, region)\n",
    "    for file_name in os.listdir(region_folder):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(region_folder, file_name)\n",
    "            site_name = file_name.split('-')[1].replace('.csv', '')\n",
    "            \n",
    "            # Print the path for debugging\n",
    "            print(f\"Reading data from: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "                # 读取并删除第一行标题\n",
    "                data_df = pd.read_csv(file_path, skiprows=1)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            data_df.columns = data_df.columns.str.strip()  # 去除列名中的空格\n",
    "            data_df['End Date'] = pd.to_datetime(data_df['End Date'], format='%d/%m/%Y', errors='coerce')\n",
    "            \n",
    "            # Filter out rows with invalid dates\n",
    "            data_df = data_df.dropna(subset=['End Date'])\n",
    "            \n",
    "            # data_df['End Time'] = pd.to_timedelta(data_df['End Time'])\n",
    "            # data_df['ReadingDateTime'] = data_df['End Date'] + data_df['End Time']\n",
    "            data_df['ReadingDateTime'] = data_df['End Date']\n",
    "            \n",
    "            data_df.set_index('ReadingDateTime', inplace=True)\n",
    "            \n",
    "            # 计算 PM2.5 的每日平均值\n",
    "            pm25_daily = data_df['PM25'].resample('D').mean().reset_index()\n",
    "            pm25_daily['Site'] = site_name\n",
    "            pm25_daily.rename(columns={'PM25': 'Value'}, inplace=True)\n",
    "            pm25_data.append(pm25_daily[['Site', 'ReadingDateTime', 'Value']])\n",
    "            \n",
    "            # 计算 PM10 的每日平均值\n",
    "            pm10_daily = data_df['PM10'].resample('D').mean().reset_index()\n",
    "            pm10_daily['Site'] = site_name\n",
    "            pm10_daily.rename(columns={'PM10': 'Value'}, inplace=True)\n",
    "            pm10_data.append(pm10_daily[['Site', 'ReadingDateTime', 'Value']])\n",
    "    \n",
    "    # 合并所有站点的 PM2.5 数据并按时间顺序排序\n",
    "    pm25_df = pd.concat(pm25_data)\n",
    "    pm25_df['ReadingDateTime'] = pd.to_datetime(pm25_df['ReadingDateTime'], format='%d/%m/%Y')\n",
    "    pm25_df = pm25_df.sort_values(by=['Site', 'ReadingDateTime'])\n",
    "    pm25_df['ReadingDateTime'] = pm25_df['ReadingDateTime'].dt.strftime('%d/%m/%Y')\n",
    "    pm25_save_path = os.path.join(save_folder, f'{region}-PM2.5 Particulates (reference equivalent).csv')\n",
    "    pm25_df.to_csv(pm25_save_path, index=False)\n",
    "    \n",
    "    # 合并所有站点的 PM10 数据并按时间顺序排序\n",
    "    pm10_df = pd.concat(pm10_data)\n",
    "    pm10_df['ReadingDateTime'] = pd.to_datetime(pm10_df['ReadingDateTime'], format='%d/%m/%Y')\n",
    "    pm10_df = pm10_df.sort_values(by=['Site', 'ReadingDateTime'])\n",
    "    pm10_df['ReadingDateTime'] = pm10_df['ReadingDateTime'].dt.strftime('%d/%m/%Y')\n",
    "    pm10_save_path = os.path.join(save_folder, f'{region}-PM10 Particulates (reference equivalent).csv')\n",
    "    pm10_df.to_csv(pm10_save_path, index=False)\n",
    "    \n",
    "    print(f\"Processed PM2.5 data saved to {pm25_save_path}\")\n",
    "    print(f\"Processed PM10 data saved to {pm10_save_path}\")\n",
    "\n",
    "# 设置基础文件夹和地区\n",
    "base_folder = 'D:\\\\File_auto\\\\0_UCL_CASA\\\\OneDrive - University College London\\\\Xiaoyi_dissertation\\\\Analysis\\\\Data\\\\AirQuality\\\\AQE'\n",
    "save_folder = 'D:\\\\File_auto\\\\0_UCL_CASA\\\\OneDrive - University College London\\\\Xiaoyi_dissertation\\\\Analysis\\\\Data\\\\AirQuality\\\\LondonAir\\\\Kensington and Chelsea'\n",
    "\n",
    "region = 'Kensington and Chelsea'\n",
    "\n",
    "# 读取和处理 AQE 数据\n",
    "read_and_process_aqe_data(base_folder, region, save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Islington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\Air on the Green_PM2.5.csv\n",
      "Successfully read D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\Air on the Green_PM2.5.csv\n",
      "Reading data from: D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\Canonbury Primary_PM2.5.csv\n",
      "Successfully read D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\Canonbury Primary_PM2.5.csv\n",
      "Reading data from: D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\Finsbury Park_PM2.5.csv\n",
      "Successfully read D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\Finsbury Park_PM2.5.csv\n",
      "Reading data from: D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\Offord Road Green Alliance_PM2.5.csv\n",
      "Successfully read D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\Offord Road Green Alliance_PM2.5.csv\n",
      "Reading data from: D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\St John's Upper Holloway CE Primary School_PM2.5.csv\n",
      "Successfully read D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\St John's Upper Holloway CE Primary School_PM2.5.csv\n",
      "Reading data from: D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\Whitehall Park School_PM2.5.csv\n",
      "Successfully read D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\BreatheLondon2\\Islington\\Whitehall Park School_PM2.5.csv\n",
      "Processed PM2.5 data saved to D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\LondonAir\\Islington\\Islington-PM2.5 Particulates (reference equivalent).csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_and_process_pm25_data(base_folder, region, save_folder):\n",
    "    region_folder = os.path.join(base_folder, region)\n",
    "    pm25_data = []\n",
    "    \n",
    "    for file_name in os.listdir(region_folder):\n",
    "        if file_name.endswith('_PM2.5.csv'):\n",
    "            site_name = file_name.replace('_PM2.5.csv', '')\n",
    "            file_path = os.path.join(region_folder, file_name)\n",
    "            \n",
    "            # Print the path for debugging\n",
    "            print(f\"Reading data from: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "                data_df = pd.read_csv(file_path)\n",
    "                print(f\"Successfully read {file_path}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            if 'Date' in data_df.columns and 'PM2.5_mean' in data_df.columns:\n",
    "                data_df['Date'] = pd.to_datetime(data_df['Date'], format='%Y-%m-%d', errors='coerce')\n",
    "                \n",
    "                # # Filter out rows with invalid dates\n",
    "                # data_df = data_df.dropna(subset=['Date'])\n",
    "                \n",
    "                if data_df.empty:\n",
    "                    print(f\"No valid data in {file_path} after filtering dates\")\n",
    "                    continue\n",
    "                \n",
    "                data_df['Site'] = site_name\n",
    "                data_df.rename(columns={'Date': 'ReadingDateTime', 'PM2.5_mean': 'Value'}, inplace=True)\n",
    "                pm25_data.append(data_df[['Site', 'ReadingDateTime', 'Value']])\n",
    "            else:\n",
    "                print(f\"Required columns not found in {file_path}\")\n",
    "    \n",
    "    if not pm25_data:\n",
    "        print(\"No data to process\")\n",
    "        return\n",
    "    \n",
    "    # 合并所有站点的 PM2.5 数据并按时间顺序排序\n",
    "    pm25_df = pd.concat(pm25_data)\n",
    "    pm25_df = pm25_df.sort_values(by=['Site', 'ReadingDateTime'])\n",
    "    pm25_df['ReadingDateTime'] = pm25_df['ReadingDateTime'].dt.strftime('%d/%m/%Y')\n",
    "    pm25_save_path = os.path.join(save_folder, f'{region}-PM2.5 Particulates (reference equivalent).csv')\n",
    "    pm25_df.to_csv(pm25_save_path, index=False)\n",
    "    \n",
    "    print(f\"Processed PM2.5 data saved to {pm25_save_path}\")\n",
    "\n",
    "# 设置基础文件夹和地区\n",
    "base_folder = 'D:\\\\File_auto\\\\0_UCL_CASA\\\\OneDrive - University College London\\\\Xiaoyi_dissertation\\\\Analysis\\\\Data\\\\AirQuality\\\\BreatheLondon2'\n",
    "save_folder = 'D:\\\\File_auto\\\\0_UCL_CASA\\\\OneDrive - University College London\\\\Xiaoyi_dissertation\\\\Analysis\\\\Data\\\\AirQuality\\\\LondonAir\\\\Islington'\n",
    "\n",
    "region = 'Islington'\n",
    "\n",
    "# 读取和处理 PM2.5 数据\n",
    "read_and_process_pm25_data(base_folder, region, save_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
