{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read and filter the original files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\dft_rawcount_local_authority_id_103_filtered.csv\n",
      "Filtered data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\dft_rawcount_local_authority_id_107_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2204947645.py:13: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='ISO-8859-1')  # 调整编码如果需要\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\dft_rawcount_local_authority_id_109_filtered.csv\n",
      "Filtered data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\dft_rawcount_local_authority_id_110_filtered.csv\n",
      "Filtered data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\dft_rawcount_local_authority_id_145_filtered.csv\n",
      "Filtered data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\dft_rawcount_local_authority_id_174_filtered.csv\n",
      "Filtered data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\dft_rawcount_local_authority_id_96_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "# File path to the folder containing the original CSV files\n",
    "dfl_file_path = 'N:/MSc_grad_proj/Data/dfl_traffic_count'\n",
    "years_to_filter = [2018, 2019, 2021, 2022, 2023, 2024]\n",
    "\n",
    "# List all files in the directory\n",
    "file_paths = [os.path.join(dfl_file_path, file) for file in os.listdir(dfl_file_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize list to store filtered file paths\n",
    "file_paths_filtered = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')  # 调整编码如果需要\n",
    "    \n",
    "    # 将'count_date'列转换为datetime类型\n",
    "    df['count_date'] = pd.to_datetime(df['count_date'], errors='coerce')\n",
    "    \n",
    "    # 删除'all_motor_vehicles'等于0的行\n",
    "    df = df[df['all_motor_vehicles'] != 0]\n",
    "    \n",
    "    # 过滤指定年份的行\n",
    "    filtered_df = df[df['count_date'].dt.year.isin(years_to_filter)]\n",
    "    \n",
    "    # 保存过滤后的数据到新文件\n",
    "    new_file_path = file_path.replace('.csv', '_filtered.csv')\n",
    "    filtered_df.to_csv(new_file_path, index=False)\n",
    "    print(f\"Filtered data saved to {new_file_path}\")\n",
    "    \n",
    "    # Append new file path to the filtered file paths list\n",
    "    file_paths_filtered.append(new_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N:\\\\MSc_grad_proj\\\\Data\\\\dfl_traffic_count\\\\dft_rawcount_local_authority_id_103.csv',\n",
       " 'N:\\\\MSc_grad_proj\\\\Data\\\\dfl_traffic_count\\\\dft_rawcount_local_authority_id_107.csv',\n",
       " 'N:\\\\MSc_grad_proj\\\\Data\\\\dfl_traffic_count\\\\dft_rawcount_local_authority_id_109.csv',\n",
       " 'N:\\\\MSc_grad_proj\\\\Data\\\\dfl_traffic_count\\\\dft_rawcount_local_authority_id_110.csv',\n",
       " 'N:\\\\MSc_grad_proj\\\\Data\\\\dfl_traffic_count\\\\dft_rawcount_local_authority_id_145.csv',\n",
       " 'N:\\\\MSc_grad_proj\\\\Data\\\\dfl_traffic_count\\\\dft_rawcount_local_authority_id_174.csv',\n",
       " 'N:\\\\MSc_grad_proj\\\\Data\\\\dfl_traffic_count\\\\dft_rawcount_local_authority_id_96.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N:/MSc_grad_proj/Data/dfl_traffic_count\\\\dft_rawcount_local_authority_id_103_filtered.csv',\n",
       " 'N:/MSc_grad_proj/Data/dfl_traffic_count\\\\dft_rawcount_local_authority_id_107_filtered.csv',\n",
       " 'N:/MSc_grad_proj/Data/dfl_traffic_count\\\\dft_rawcount_local_authority_id_109_filtered.csv',\n",
       " 'N:/MSc_grad_proj/Data/dfl_traffic_count\\\\dft_rawcount_local_authority_id_110_filtered.csv',\n",
       " 'N:/MSc_grad_proj/Data/dfl_traffic_count\\\\dft_rawcount_local_authority_id_145_filtered.csv',\n",
       " 'N:/MSc_grad_proj/Data/dfl_traffic_count\\\\dft_rawcount_local_authority_id_174_filtered.csv',\n",
       " 'N:/MSc_grad_proj/Data/dfl_traffic_count\\\\dft_rawcount_local_authority_id_96_filtered.csv']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Processing filtered files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2802831072.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2802831072.py:16: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('D').mean()  # Resample to daily frequency and calculate mean for existing data\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of ['count_date'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mn:\\MSc_grad_proj\\Interpolation\\0807traffic_raw_count.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m road_type \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mroad_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique():\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     road_df \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mroad_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m road_type]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     interpolated_df \u001b[39m=\u001b[39m linear_interpolation(road_df, \u001b[39m'\u001b[39;49m\u001b[39mcount_date\u001b[39;49m\u001b[39m'\u001b[39;49m, value_columns)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X12sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# Calculate 'all_motor_vehicles' as the sum of specific columns\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X12sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     interpolated_df[\u001b[39m'\u001b[39m\u001b[39mall_motor_vehicles\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m interpolated_df[value_columns]\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mn:\\MSc_grad_proj\\Interpolation\\0807traffic_raw_count.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m full_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(date_range, columns\u001b[39m=\u001b[39m[date_col])\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m value_cols:\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Perform linear interpolation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     df\u001b[39m.\u001b[39;49mset_index(date_col, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mresample(\u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmean()  \u001b[39m# Resample to daily frequency and calculate mean for existing data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     df[col] \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39minterpolate(method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6012\u001b[0m, in \u001b[0;36mDataFrame.set_index\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   6009\u001b[0m                 missing\u001b[39m.\u001b[39mappend(col)\n\u001b[0;32m   6011\u001b[0m \u001b[39mif\u001b[39;00m missing:\n\u001b[1;32m-> 6012\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of \u001b[39m\u001b[39m{\u001b[39;00mmissing\u001b[39m}\u001b[39;00m\u001b[39m are in the columns\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6014\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   6015\u001b[0m     frame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of ['count_date'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# Function to perform linear interpolation\n",
    "def linear_interpolation(df, date_col, value_cols):\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    min_date = df[date_col].min()\n",
    "    max_date = df[date_col].max()\n",
    "    \n",
    "    # Create a complete date range\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    \n",
    "    # Create a new DataFrame with the complete date range\n",
    "    full_df = pd.DataFrame(date_range, columns=[date_col])\n",
    "    \n",
    "    for col in value_cols:\n",
    "        # Perform linear interpolation\n",
    "        df.set_index(date_col, inplace=True)\n",
    "        df = df.resample('D').mean()  # Resample to daily frequency and calculate mean for existing data\n",
    "        df[col] = df[col].interpolate(method='linear')\n",
    "        \n",
    "        # Ensure interpolated values do not fall below the minimum original values\n",
    "        min_value = df[col].min()\n",
    "        df[col] = np.where(df[col] < min_value, min_value, df[col])\n",
    "        \n",
    "        # Round values to 2 decimal places\n",
    "        df[col] = df[col].round(2)\n",
    "        \n",
    "        full_df = full_df.merge(df[col], left_on=date_col, right_on=date_col, how='left')\n",
    "    \n",
    "    return full_df\n",
    "\n",
    "# Specify the columns to interpolate\n",
    "value_columns = ['pedal_cycles', 'two_wheeled_motor_vehicles', 'cars_and_taxis', 'buses_and_coaches',\n",
    "                 'lgvs', 'hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', 'hgvs_4_or_more_rigid_axle',\n",
    "                 'hgvs_3_or_4_articulated_axle', 'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle',\n",
    "                 'all_hgvs']\n",
    "\n",
    "for file_path in file_paths_filtered:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Interpolate separately for each road type\n",
    "    for road_type in df['road_type'].unique():\n",
    "        road_df = df[df['road_type'] == road_type]\n",
    "        interpolated_df = linear_interpolation(road_df, 'count_date', value_columns)\n",
    "        \n",
    "        # Calculate 'all_motor_vehicles' as the sum of specific columns\n",
    "        interpolated_df['all_motor_vehicles'] = interpolated_df[value_columns].sum(axis=1)\n",
    "        \n",
    "        # Merge interpolated data back into the main DataFrame\n",
    "        interpolated_df['road_type'] = road_type\n",
    "        if road_type == df['road_type'].unique()[0]:\n",
    "            full_interpolated_df = interpolated_df\n",
    "        else:\n",
    "            full_interpolated_df = pd.concat([full_interpolated_df, interpolated_df])\n",
    "    \n",
    "    # Construct new file path\n",
    "    processed_file_path = file_path.replace('dft_rawcount_local_authority_id_', '')\n",
    "    processed_file_path = processed_file_path.replace('145', 'Camden').replace('174', 'City of London').replace('96', 'Islington').replace('110', 'Kensington and Chelsea').replace('107', 'Lambeth').replace('103', 'Southwark').replace('109', 'Westminster')\n",
    "    \n",
    "    # Save the interpolated data back to a new CSV file\n",
    "    full_interpolated_df.to_csv(processed_file_path, index=False)\n",
    "    print(f\"Processed data saved to {processed_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Interpolation（多项式插值）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Interpolation for Each Road Type Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\Southwark_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\Lambeth_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\Westminster_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\Kensington and Chelsea_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to N:/MSc_grad_proj/Data/dfl_traffic_count\\Camden_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\2026349070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "c:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\lib\\polynomial.py:667: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge in Linear Least Squares",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mn:\\MSc_grad_proj\\Interpolation\\0807traffic_raw_count.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X23sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mfor\u001b[39;00m road_type \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mroad_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique():\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X23sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     road_df \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mroad_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m road_type]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X23sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     interpolated_df \u001b[39m=\u001b[39m polynomial_interpolation(road_df, \u001b[39m'\u001b[39;49m\u001b[39mcount_date\u001b[39;49m\u001b[39m'\u001b[39;49m, value_columns, max_degree\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X23sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m# Calculate 'all_motor_vehicles' as the sum of specific columns\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X23sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     interpolated_df[\u001b[39m'\u001b[39m\u001b[39mall_motor_vehicles\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m interpolated_df[value_columns]\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mn:\\MSc_grad_proj\\Interpolation\\0807traffic_raw_count.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     full_df[col] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m polynomial \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpoly1d(np\u001b[39m.\u001b[39;49mpolyfit(x, y, degree))\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Apply the polynomial to the full date range\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X23sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m full_df[col] \u001b[39m=\u001b[39m polynomial((full_df[date_col] \u001b[39m-\u001b[39m min_date)\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mdays\u001b[39m.\u001b[39mvalues)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mpolyfit\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\lib\\polynomial.py:668\u001b[0m, in \u001b[0;36mpolyfit\u001b[1;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[0;32m    666\u001b[0m scale \u001b[39m=\u001b[39m NX\u001b[39m.\u001b[39msqrt((lhs\u001b[39m*\u001b[39mlhs)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m    667\u001b[0m lhs \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m scale\n\u001b[1;32m--> 668\u001b[0m c, resids, rank, s \u001b[39m=\u001b[39m lstsq(lhs, rhs, rcond)\n\u001b[0;32m    669\u001b[0m c \u001b[39m=\u001b[39m (c\u001b[39m.\u001b[39mT\u001b[39m/\u001b[39mscale)\u001b[39m.\u001b[39mT  \u001b[39m# broadcast scale coefficients\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[39m# warn on rank reduction, which indicates an ill conditioned matrix\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mlstsq\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:2300\u001b[0m, in \u001b[0;36mlstsq\u001b[1;34m(a, b, rcond)\u001b[0m\n\u001b[0;32m   2297\u001b[0m \u001b[39mif\u001b[39;00m n_rhs \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2298\u001b[0m     \u001b[39m# lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\u001b[39;00m\n\u001b[0;32m   2299\u001b[0m     b \u001b[39m=\u001b[39m zeros(b\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (m, n_rhs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mb\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m-> 2300\u001b[0m x, resids, rank, s \u001b[39m=\u001b[39m gufunc(a, b, rcond, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[0;32m   2301\u001b[0m \u001b[39mif\u001b[39;00m m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2302\u001b[0m     x[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:101\u001b[0m, in \u001b[0;36m_raise_linalgerror_lstsq\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_lstsq\u001b[39m(err, flag):\n\u001b[1;32m--> 101\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSVD did not converge in Linear Least Squares\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: SVD did not converge in Linear Least Squares"
     ]
    }
   ],
   "source": [
    "# Function to perform polynomial interpolation\n",
    "def polynomial_interpolation(df, date_col, value_cols, max_degree=3):\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    min_date = df[date_col].min()\n",
    "    max_date = df[date_col].max()\n",
    "    \n",
    "    # Create a complete date range\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    \n",
    "    # Create a new DataFrame with the complete date range\n",
    "    full_df = pd.DataFrame(date_range, columns=[date_col])\n",
    "    \n",
    "    for col in value_cols:\n",
    "        # Fit a polynomial to the non-null data\n",
    "        non_null_data = df.dropna(subset=[col])\n",
    "        x = (non_null_data[date_col] - min_date).dt.days.values\n",
    "        y = non_null_data[col].values\n",
    "        \n",
    "        # Adjust polynomial degree based on the number of data points\n",
    "        degree = min(max_degree, len(x) - 1)\n",
    "        if degree < 1:\n",
    "            full_df[col] = np.nan\n",
    "            continue\n",
    "        \n",
    "        polynomial = np.poly1d(np.polyfit(x, y, degree))\n",
    "        \n",
    "        # Apply the polynomial to the full date range\n",
    "        full_df[col] = polynomial((full_df[date_col] - min_date).dt.days.values)\n",
    "        \n",
    "        # Ensure interpolated values do not fall below the minimum original values\n",
    "        min_value = non_null_data[col].min()\n",
    "        full_df[col] = np.where(full_df[col] < min_value, min_value, full_df[col])\n",
    "        \n",
    "        # Round values to 2 decimal places\n",
    "        full_df[col] = full_df[col].round(2)\n",
    "    \n",
    "    return full_df\n",
    "\n",
    "# Specify the columns to interpolate\n",
    "value_columns = ['pedal_cycles', 'two_wheeled_motor_vehicles', 'cars_and_taxis', 'buses_and_coaches',\n",
    "                 'lgvs', 'hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', 'hgvs_4_or_more_rigid_axle',\n",
    "                 'hgvs_3_or_4_articulated_axle', 'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle',\n",
    "                 'all_hgvs']\n",
    "\n",
    "for file_path in file_paths_filtered:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Interpolate separately for each road type\n",
    "    for road_type in df['road_type'].unique():\n",
    "        road_df = df[df['road_type'] == road_type]\n",
    "        interpolated_df = polynomial_interpolation(road_df, 'count_date', value_columns, max_degree=3)\n",
    "        \n",
    "        # Calculate 'all_motor_vehicles' as the sum of specific columns\n",
    "        interpolated_df['all_motor_vehicles'] = interpolated_df[value_columns].sum(axis=1)\n",
    "        \n",
    "        # Merge interpolated data back into the main DataFrame\n",
    "        interpolated_df['road_type'] = road_type\n",
    "        if road_type == df['road_type'].unique()[0]:\n",
    "            full_interpolated_df = interpolated_df\n",
    "        else:\n",
    "            full_interpolated_df = pd.concat([full_interpolated_df, interpolated_df])\n",
    "    \n",
    "    # Construct new file path\n",
    "    processed_file_path = file_path.replace('dft_rawcount_local_authority_id_', '')\n",
    "    processed_file_path = processed_file_path.replace('145', 'Camden').replace('174', 'City of London').replace('96', 'Islington').replace('110', 'Kensington and Chelsea').replace('107', 'Lambeth').replace('103', 'Southwark').replace('109', 'Westminster')\n",
    "    \n",
    "    # Save the interpolated data back to a new CSV file\n",
    "    full_interpolated_df.to_csv(processed_file_path, index=False)\n",
    "    print(f\"Processed data saved to {processed_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform polynomial interpolation\n",
    "def polynomial_interpolation(df, date_col, value_cols, degree=3):\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    min_date = df[date_col].min()\n",
    "    max_date = df[date_col].max()\n",
    "    \n",
    "    # Create a complete date range\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    \n",
    "    # Create a new DataFrame with the complete date range\n",
    "    full_df = pd.DataFrame(date_range, columns=[date_col])\n",
    "    \n",
    "    for col in value_cols:\n",
    "        # Fit a polynomial to the non-null data\n",
    "        non_null_data = df.dropna(subset=[col])\n",
    "        x = (non_null_data[date_col] - min_date).dt.days.values\n",
    "        y = non_null_data[col].values\n",
    "        polynomial = np.poly1d(np.polyfit(x, y, degree))\n",
    "        \n",
    "        # Apply the polynomial to the full date range\n",
    "        full_df[col] = polynomial((full_df[date_col] - min_date).dt.days.values)\n",
    "        \n",
    "        # Ensure interpolated values do not fall below the minimum original values\n",
    "        min_value = non_null_data[col].min()\n",
    "        full_df[col] = np.where(full_df[col] < min_value, min_value, full_df[col])\n",
    "        \n",
    "        # Round values to 2 decimal places\n",
    "        full_df[col] = full_df[col].round(2)\n",
    "    \n",
    "    return full_df\n",
    "\n",
    "# Specify the columns to interpolate\n",
    "value_columns = ['pedal_cycles', 'two_wheeled_motor_vehicles', 'cars_and_taxis', 'buses_and_coaches',\n",
    "                 'lgvs', 'hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', 'hgvs_4_or_more_rigid_axle',\n",
    "                 'hgvs_3_or_4_articulated_axle', 'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle',\n",
    "                 'all_hgvs']\n",
    "\n",
    "for file_path in file_paths_filtered:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Interpolate separately for each road type\n",
    "    for road_type in df['road_type'].unique():\n",
    "        road_df = df[df['road_type'] == road_type]\n",
    "        interpolated_df = polynomial_interpolation(road_df, 'count_date', value_columns, degree=3)\n",
    "        \n",
    "        # Calculate 'all_motor_vehicles' as the sum of specific columns\n",
    "        interpolated_df['all_motor_vehicles'] = interpolated_df[value_columns].sum(axis=1)\n",
    "        \n",
    "        # Merge interpolated data back into the main DataFrame\n",
    "        interpolated_df['road_type'] = road_type\n",
    "        if road_type == df['road_type'].unique()[0]:\n",
    "            full_interpolated_df = interpolated_df\n",
    "        else:\n",
    "            full_interpolated_df = pd.concat([full_interpolated_df, interpolated_df])\n",
    "    \n",
    "    # Construct new file path\n",
    "    processed_file_path = file_path.replace('.csv', '_processed.csv')\n",
    "    processed_file_path = processed_file_path.replace('dft_rawcount_local_authority_id_', '')\n",
    "    processed_file_path = processed_file_path.replace('145', 'Camden').replace('174', 'City of London').replace('96', 'Islington').replace('110', 'Kensington and Chelsea').replace('107', 'Lambeth').replace('103', 'Southwark').replace('109', 'Westminster')\n",
    "    \n",
    "    # Save the interpolated data back to a new CSV file\n",
    "    full_interpolated_df.to_csv(processed_file_path, index=False)\n",
    "    print(f\"Processed data saved to {processed_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spline interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\3818128250.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\ucfnxch\\AppData\\Local\\Temp\\ipykernel_1708\\3818128250.py:16: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('D').mean()  # Resample to daily frequency and calculate mean for existing data\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of ['count_date'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mn:\\MSc_grad_proj\\Interpolation\\0807traffic_raw_count.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m road_type \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mroad_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique():\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X22sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     road_df \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mroad_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m road_type]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     interpolated_df \u001b[39m=\u001b[39m spline_interpolation(road_df, \u001b[39m'\u001b[39;49m\u001b[39mcount_date\u001b[39;49m\u001b[39m'\u001b[39;49m, value_columns)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X22sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# Calculate 'all_motor_vehicles' as the sum of specific columns\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X22sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     interpolated_df[\u001b[39m'\u001b[39m\u001b[39mall_motor_vehicles\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m interpolated_df[value_columns]\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mn:\\MSc_grad_proj\\Interpolation\\0807traffic_raw_count.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m full_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(date_range, columns\u001b[39m=\u001b[39m[date_col])\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m value_cols:\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Perform spline interpolation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     df\u001b[39m.\u001b[39;49mset_index(date_col, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mresample(\u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmean()  \u001b[39m# Resample to daily frequency and calculate mean for existing data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/MSc_grad_proj/Interpolation/0807traffic_raw_count.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     df[col] \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39minterpolate(method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mspline\u001b[39m\u001b[39m'\u001b[39m, order\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6012\u001b[0m, in \u001b[0;36mDataFrame.set_index\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   6009\u001b[0m                 missing\u001b[39m.\u001b[39mappend(col)\n\u001b[0;32m   6011\u001b[0m \u001b[39mif\u001b[39;00m missing:\n\u001b[1;32m-> 6012\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of \u001b[39m\u001b[39m{\u001b[39;00mmissing\u001b[39m}\u001b[39;00m\u001b[39m are in the columns\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6014\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   6015\u001b[0m     frame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of ['count_date'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# Function to perform spline interpolation\n",
    "def spline_interpolation(df, date_col, value_cols):\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    min_date = df[date_col].min()\n",
    "    max_date = df[date_col].max()\n",
    "    \n",
    "    # Create a complete date range\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    \n",
    "    # Create a new DataFrame with the complete date range\n",
    "    full_df = pd.DataFrame(date_range, columns=[date_col])\n",
    "    \n",
    "    for col in value_cols:\n",
    "        # Perform spline interpolation\n",
    "        df.set_index(date_col, inplace=True)\n",
    "        df = df.resample('D').mean()  # Resample to daily frequency and calculate mean for existing data\n",
    "        df[col] = df[col].interpolate(method='spline', order=3)\n",
    "        \n",
    "        # Ensure interpolated values do not fall below the minimum original values\n",
    "        min_value = df[col].min()\n",
    "        df[col] = np.where(df[col] < min_value, min_value, df[col])\n",
    "        \n",
    "        # Round values to 2 decimal places\n",
    "        df[col] = df[col].round(2)\n",
    "        \n",
    "        full_df = full_df.merge(df[col], left_on=date_col, right_on=date_col, how='left')\n",
    "    \n",
    "    return full_df\n",
    "\n",
    "# Specify the columns to interpolate\n",
    "value_columns = ['pedal_cycles', 'two_wheeled_motor_vehicles', 'cars_and_taxis', 'buses_and_coaches',\n",
    "                 'lgvs', 'hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', 'hgvs_4_or_more_rigid_axle',\n",
    "                 'hgvs_3_or_4_articulated_axle', 'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle',\n",
    "                 'all_hgvs']\n",
    "\n",
    "for file_path in file_paths_filtered:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Interpolate separately for each road type\n",
    "    for road_type in df['road_type'].unique():\n",
    "        road_df = df[df['road_type'] == road_type]\n",
    "        interpolated_df = spline_interpolation(road_df, 'count_date', value_columns)\n",
    "        \n",
    "        # Calculate 'all_motor_vehicles' as the sum of specific columns\n",
    "        interpolated_df['all_motor_vehicles'] = interpolated_df[value_columns].sum(axis=1)\n",
    "        \n",
    "        # Merge interpolated data back into the main DataFrame\n",
    "        interpolated_df['road_type'] = road_type\n",
    "        if road_type == df['road_type'].unique()[0]:\n",
    "            full_interpolated_df = interpolated_df\n",
    "        else:\n",
    "            full_interpolated_df = pd.concat([full_interpolated_df, interpolated_df])\n",
    "    \n",
    "    # Construct new file path\n",
    "    processed_file_path = file_path.replace('.csv', '_processed.csv')\n",
    "    processed_file_path = processed_file_path.replace('dft_rawcount_local_authority_id_', '')\n",
    "    processed_file_path = processed_file_path.replace('145', 'Camden').replace('174', 'City of London').replace('96', 'Islington').replace('110', 'Kensington and Chelsea').replace('107', 'Lambeth').replace('103', 'Southwark').replace('109', 'Westminster')\n",
    "    \n",
    "    # Save the interpolated data back to a new CSV file\n",
    "    full_interpolated_df.to_csv(processed_file_path, index=False)\n",
    "    print(f\"Processed data saved to {processed_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Step 2: Process the filtered files\n",
    "filtered_file_paths = [file.replace('.csv', '_filtered.csv') for file in file_paths]\n",
    "\n",
    "for filtered_file_path in filtered_file_paths:\n",
    "    # Check if filtered file exists\n",
    "    if not os.path.exists(filtered_file_path):\n",
    "        print(f\"Filtered file {filtered_file_path} does not exist. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Read the filtered CSV file\n",
    "    df = pd.read_csv(filtered_file_path, encoding='ISO-8859-1')  # Adjust encoding if necessary\n",
    "    \n",
    "    # Convert 'count_date' to datetime and extract the date part\n",
    "    df['count_date'] = pd.to_datetime(df['count_date'], errors='coerce')\n",
    "    df['date'] = df['count_date'].dt.date\n",
    "    \n",
    "    # Group by day and calculate the mean for specific columns\n",
    "    columns_to_interpolate = [\n",
    "        'pedal_cycles', 'two_wheeled_motor_vehicles', 'cars_and_taxis', \n",
    "        'buses_and_coaches', 'lgvs', 'hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', \n",
    "        'hgvs_4_or_more_rigid_axle', 'hgvs_3_or_4_articulated_axle', \n",
    "        'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle', 'all_hgvs', \n",
    "        'all_motor_vehicles'\n",
    "    ]\n",
    "    daily_df = df.groupby('date')[columns_to_interpolate].mean().reset_index()\n",
    "    \n",
    "    # Create a complete date range\n",
    "    full_date_range = pd.date_range(start=daily_df['date'].min(), end=daily_df['date'].max())\n",
    "    \n",
    "    # Reindex to have a continuous date range\n",
    "    daily_df = daily_df.set_index('date').reindex(full_date_range).reset_index()\n",
    "    daily_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "    \n",
    "    # Interpolate missing values using spline interpolation for specified columns\n",
    "    for column in columns_to_interpolate:\n",
    "        mask = np.isfinite(daily_df[column])\n",
    "        x = np.arange(len(daily_df))\n",
    "        spline = UnivariateSpline(x[mask], daily_df.loc[mask, column], s=0)\n",
    "        daily_df[column] = spline(x)\n",
    "        daily_df[column] = daily_df[column].round(2)  # Round to 2 decimal places\n",
    "    \n",
    "    # Save the processed data to a new CSV file\n",
    "    processed_file_path = filtered_file_path.replace('_filtered.csv', '_processed.csv')\n",
    "    daily_df.to_csv(processed_file_path, index=False)\n",
    "    \n",
    "    print(f\"Processed data saved to {processed_file_path}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
