{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read and filter the original files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to dfl_traffic_count\\dft_rawcount_local_authority_id_103_filtered.csv\n",
      "Filtered data saved to dfl_traffic_count\\dft_rawcount_local_authority_id_107_filtered.csv\n",
      "Filtered data saved to dfl_traffic_count\\dft_rawcount_local_authority_id_109_filtered.csv\n",
      "Filtered data saved to dfl_traffic_count\\dft_rawcount_local_authority_id_110_filtered.csv\n",
      "Filtered data saved to dfl_traffic_count\\dft_rawcount_local_authority_id_145_filtered.csv\n",
      "Filtered data saved to dfl_traffic_count\\dft_rawcount_local_authority_id_174_filtered.csv\n",
      "Filtered data saved to dfl_traffic_count\\dft_rawcount_local_authority_id_96_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\1903908866.py:13: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='ISO-8859-1')  # 调整编码如果需要\n"
     ]
    }
   ],
   "source": [
    "# File path to the folder containing the original CSV files\n",
    "dfl_file_path = 'dfl_traffic_count'\n",
    "years_to_filter = [2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "\n",
    "# List all files in the directory\n",
    "file_paths = [os.path.join(dfl_file_path, file) for file in os.listdir(dfl_file_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize list to store filtered file paths\n",
    "file_paths_filtered = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')  # 调整编码如果需要\n",
    "    \n",
    "    # 将'count_date'列转换为datetime类型\n",
    "    df['count_date'] = pd.to_datetime(df['count_date'], errors='coerce')\n",
    "    \n",
    "    # 删除'all_motor_vehicles'等于0的行\n",
    "    df = df[df['all_motor_vehicles'] != 0]\n",
    "    \n",
    "    # 过滤指定年份的行\n",
    "    filtered_df = df[df['count_date'].dt.year.isin(years_to_filter)]\n",
    "    \n",
    "    # 保存过滤后的数据到新文件\n",
    "    new_file_path = file_path.replace('.csv', '_filtered.csv')\n",
    "    filtered_df.to_csv(new_file_path, index=False)\n",
    "    print(f\"Filtered data saved to {new_file_path}\")\n",
    "    \n",
    "    # Append new file path to the filtered file paths list\n",
    "    file_paths_filtered.append(new_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dfl_traffic_count\\\\dft_rawcount_local_authority_id_103.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_107.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_109.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_110.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_145.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_174.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_96.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dfl_traffic_count\\\\dft_rawcount_local_authority_id_103_filtered.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_107_filtered.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_109_filtered.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_110_filtered.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_145_filtered.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_174_filtered.csv',\n",
       " 'dfl_traffic_count\\\\dft_rawcount_local_authority_id_96_filtered.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Processing filtered files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Interpolation（多项式插值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to dfl_traffic_count\\dft_rawcount_Southwark.csv\n",
      "Processed data saved to dfl_traffic_count\\dft_rawcount_Lambeth.csv\n",
      "Processed data saved to dfl_traffic_count\\dft_rawcount_Westminster.csv\n",
      "Processed data saved to dfl_traffic_count\\dft_rawcount_Kensington and Chelsea.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "e:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to dfl_traffic_count\\dft_rawcount_Camden.csv\n",
      "Failed to fit polynomial for pedal_cycles due to insufficient data.\n",
      "Failed to fit polynomial for two_wheeled_motor_vehicles due to insufficient data.\n",
      "Failed to fit polynomial for cars_and_taxis due to insufficient data.\n",
      "Failed to fit polynomial for buses_and_coaches due to insufficient data.\n",
      "Failed to fit polynomial for lgvs due to insufficient data.\n",
      "Failed to fit polynomial for hgvs_2_rigid_axle due to insufficient data.\n",
      "Failed to fit polynomial for hgvs_3_rigid_axle due to insufficient data.\n",
      "Failed to fit polynomial for hgvs_4_or_more_rigid_axle due to insufficient data.\n",
      "Failed to fit polynomial for hgvs_3_or_4_articulated_axle due to insufficient data.\n",
      "Failed to fit polynomial for hgvs_5_articulated_axle due to insufficient data.\n",
      "Failed to fit polynomial for hgvs_6_articulated_axle due to insufficient data.\n",
      "Failed to fit polynomial for all_hgvs due to insufficient data.\n",
      "Processed data saved to dfl_traffic_count\\dft_rawcount_City of London.csv\n",
      "Processed data saved to dfl_traffic_count\\dft_rawcount_Islington.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_17972\\711255950.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n"
     ]
    }
   ],
   "source": [
    "# Function to perform polynomial interpolation\n",
    "def polynomial_interpolation(df, date_col, value_cols, max_degree=3):\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    min_date = df[date_col].min()\n",
    "    max_date = df[date_col].max()\n",
    "    \n",
    "    # Create a complete date range\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    \n",
    "    # Create a new DataFrame with the complete date range\n",
    "    full_df = pd.DataFrame(date_range, columns=[date_col])\n",
    "    \n",
    "    for col in value_cols:\n",
    "        # Fit a polynomial to the non-null data\n",
    "        non_null_data = df.dropna(subset=[col])\n",
    "        x = (non_null_data[date_col] - min_date).dt.days.values\n",
    "        y = non_null_data[col].values\n",
    "        \n",
    "        # Adjust polynomial degree based on the number of data points\n",
    "        degree = min(max_degree, len(x) - 1)\n",
    "        if degree < 1:\n",
    "            full_df[col] = np.nan\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            polynomial = np.poly1d(np.polyfit(x, y, degree))\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(f\"Failed to fit polynomial for {col} due to insufficient data.\")\n",
    "            full_df[col] = np.nan\n",
    "            continue\n",
    "        \n",
    "        # Apply the polynomial to the full date range\n",
    "        full_df[col] = polynomial((full_df[date_col] - min_date).dt.days.values)\n",
    "        \n",
    "        # Ensure interpolated values do not fall below the minimum original values\n",
    "        min_value = non_null_data[col].min()\n",
    "        full_df[col] = np.where(full_df[col] < min_value, min_value, full_df[col])\n",
    "        \n",
    "        # Round values to 2 decimal places\n",
    "        full_df[col] = full_df[col].round(2)\n",
    "    \n",
    "    return full_df\n",
    "\n",
    "# Specify the columns to interpolate\n",
    "value_columns = ['pedal_cycles', 'two_wheeled_motor_vehicles', 'cars_and_taxis', 'buses_and_coaches',\n",
    "                 'lgvs', 'hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', 'hgvs_4_or_more_rigid_axle',\n",
    "                 'hgvs_3_or_4_articulated_axle', 'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle',\n",
    "                 'all_hgvs']\n",
    "\n",
    "for file_path in file_paths_filtered:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Interpolate separately for each road type\n",
    "    for road_type in df['road_type'].unique():\n",
    "        road_df = df[df['road_type'] == road_type]\n",
    "        interpolated_df = polynomial_interpolation(road_df, 'count_date', value_columns, max_degree=3)\n",
    "        \n",
    "        # Calculate 'all_hgvs' as the sum of specific columns\n",
    "        hgvs_columns = ['hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', 'hgvs_4_or_more_rigid_axle',\n",
    "                'hgvs_3_or_4_articulated_axle', 'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle']\n",
    "        interpolated_df['all_hgvs'] = interpolated_df[hgvs_columns].sum(axis=1)\n",
    "        \n",
    "        # Merge interpolated data back into the main DataFrame\n",
    "        interpolated_df['road_type'] = road_type\n",
    "        if road_type == df['road_type'].unique()[0]:\n",
    "            full_interpolated_df = interpolated_df\n",
    "        else:\n",
    "            full_interpolated_df = pd.concat([full_interpolated_df, interpolated_df])\n",
    "    \n",
    "    # Construct new file path\n",
    "    processed_file_path = file_path.replace('local_authority_id_', '')\n",
    "    processed_file_path = processed_file_path.replace('145_filtered', 'Camden').replace('174_filtered', 'City of London').replace('96_filtered', 'Islington').replace('110_filtered', 'Kensington and Chelsea').replace('107_filtered', 'Lambeth').replace('103_filtered', 'Southwark').replace('109_filtered', 'Westminster')\n",
    "    \n",
    "    # Save the interpolated data back to a new CSV file\n",
    "    full_interpolated_df.to_csv(processed_file_path, index=False)\n",
    "    print(f\"Processed data saved to {processed_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to perform linear interpolation\n",
    "# def linear_interpolation(df, date_col, value_cols):\n",
    "#     df[date_col] = pd.to_datetime(df[date_col])\n",
    "#     min_date = df[date_col].min()\n",
    "#     max_date = df[date_col].max()\n",
    "    \n",
    "#     # Create a complete date range\n",
    "#     date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    \n",
    "#     # Create a new DataFrame with the complete date range\n",
    "#     full_df = pd.DataFrame(date_range, columns=[date_col])\n",
    "    \n",
    "#     for col in value_cols:\n",
    "#         # Perform linear interpolation\n",
    "#         df.set_index(date_col, inplace=True)\n",
    "#         df = df.resample('D').mean()  # Resample to daily frequency and calculate mean for existing data\n",
    "#         df[col] = df[col].interpolate(method='linear')\n",
    "        \n",
    "#         # Ensure interpolated values do not fall below the minimum original values\n",
    "#         min_value = df[col].min()\n",
    "#         df[col] = np.where(df[col] < min_value, min_value, df[col])\n",
    "        \n",
    "#         # Round values to 2 decimal places\n",
    "#         df[col] = df[col].round(2)\n",
    "        \n",
    "#         full_df = full_df.merge(df[col], left_on=date_col, right_on=date_col, how='left')\n",
    "    \n",
    "#     return full_df\n",
    "\n",
    "# # Specify the columns to interpolate\n",
    "# value_columns = ['pedal_cycles', 'two_wheeled_motor_vehicles', 'cars_and_taxis', 'buses_and_coaches',\n",
    "#                  'lgvs', 'hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', 'hgvs_4_or_more_rigid_axle',\n",
    "#                  'hgvs_3_or_4_articulated_axle', 'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle',\n",
    "#                  'all_hgvs']\n",
    "\n",
    "# for file_path in file_paths_filtered:\n",
    "#     # Read the CSV file\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     # Interpolate separately for each road type\n",
    "#     for road_type in df['road_type'].unique():\n",
    "#         road_df = df[df['road_type'] == road_type]\n",
    "#         interpolated_df = linear_interpolation(road_df, 'count_date', value_columns)\n",
    "        \n",
    "#         # Calculate 'all_motor_vehicles' as the sum of specific columns\n",
    "#         interpolated_df['all_motor_vehicles'] = interpolated_df[value_columns].sum(axis=1)\n",
    "        \n",
    "#         # Merge interpolated data back into the main DataFrame\n",
    "#         interpolated_df['road_type'] = road_type\n",
    "#         if road_type == df['road_type'].unique()[0]:\n",
    "#             full_interpolated_df = interpolated_df\n",
    "#         else:\n",
    "#             full_interpolated_df = pd.concat([full_interpolated_df, interpolated_df])\n",
    "    \n",
    "#     # Construct new file path\n",
    "#     processed_file_path = file_path.replace('dft_rawcount_local_authority_id_', '')\n",
    "#     processed_file_path = processed_file_path.replace('145', 'Camden').replace('174', 'City of London').replace('96', 'Islington').replace('110', 'Kensington and Chelsea').replace('107', 'Lambeth').replace('103', 'Southwark').replace('109', 'Westminster')\n",
    "    \n",
    "#     # Save the interpolated data back to a new CSV file\n",
    "#     full_interpolated_df.to_csv(processed_file_path, index=False)\n",
    "#     print(f\"Processed data saved to {processed_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spline interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to perform spline interpolation\n",
    "# def spline_interpolation(df, date_col, value_cols):\n",
    "#     df[date_col] = pd.to_datetime(df[date_col])\n",
    "#     min_date = df[date_col].min()\n",
    "#     max_date = df[date_col].max()\n",
    "    \n",
    "#     # Create a complete date range\n",
    "#     date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    \n",
    "#     # Create a new DataFrame with the complete date range\n",
    "#     full_df = pd.DataFrame(date_range, columns=[date_col])\n",
    "    \n",
    "#     for col in value_cols:\n",
    "#         # Perform spline interpolation\n",
    "#         df.set_index(date_col, inplace=True)\n",
    "#         df = df.resample('D').mean()  # Resample to daily frequency and calculate mean for existing data\n",
    "#         df[col] = df[col].interpolate(method='spline', order=3)\n",
    "        \n",
    "#         # Ensure interpolated values do not fall below the minimum original values\n",
    "#         min_value = df[col].min()\n",
    "#         df[col] = np.where(df[col] < min_value, min_value, df[col])\n",
    "        \n",
    "#         # Round values to 2 decimal places\n",
    "#         df[col] = df[col].round(2)\n",
    "        \n",
    "#         full_df = full_df.merge(df[col], left_on=date_col, right_on=date_col, how='left')\n",
    "    \n",
    "#     return full_df\n",
    "\n",
    "# # Specify the columns to interpolate\n",
    "# value_columns = ['pedal_cycles', 'two_wheeled_motor_vehicles', 'cars_and_taxis', 'buses_and_coaches',\n",
    "#                  'lgvs', 'hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', 'hgvs_4_or_more_rigid_axle',\n",
    "#                  'hgvs_3_or_4_articulated_axle', 'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle',\n",
    "#                  'all_hgvs']\n",
    "\n",
    "# for file_path in file_paths_filtered:\n",
    "#     # Read the CSV file\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     # Interpolate separately for each road type\n",
    "#     for road_type in df['road_type'].unique():\n",
    "#         road_df = df[df['road_type'] == road_type]\n",
    "#         interpolated_df = spline_interpolation(road_df, 'count_date', value_columns)\n",
    "        \n",
    "#         # Calculate 'all_motor_vehicles' as the sum of specific columns\n",
    "#         interpolated_df['all_motor_vehicles'] = interpolated_df[value_columns].sum(axis=1)\n",
    "        \n",
    "#         # Merge interpolated data back into the main DataFrame\n",
    "#         interpolated_df['road_type'] = road_type\n",
    "#         if road_type == df['road_type'].unique()[0]:\n",
    "#             full_interpolated_df = interpolated_df\n",
    "#         else:\n",
    "#             full_interpolated_df = pd.concat([full_interpolated_df, interpolated_df])\n",
    "    \n",
    "#     # Construct new file path\n",
    "#     processed_file_path = file_path.replace('.csv', '_processed.csv')\n",
    "#     processed_file_path = processed_file_path.replace('dft_rawcount_local_authority_id_', '')\n",
    "#     processed_file_path = processed_file_path.replace('145', 'Camden').replace('174', 'City of London').replace('96', 'Islington').replace('110', 'Kensington and Chelsea').replace('107', 'Lambeth').replace('103', 'Southwark').replace('109', 'Westminster')\n",
    "    \n",
    "#     # Save the interpolated data back to a new CSV file\n",
    "#     full_interpolated_df.to_csv(processed_file_path, index=False)\n",
    "#     print(f\"Processed data saved to {processed_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Step 2: Process the filtered files\\nfiltered_file_paths = [file.replace(\\'.csv\\', \\'_filtered.csv\\') for file in file_paths]\\n\\nfor filtered_file_path in filtered_file_paths:\\n    # Check if filtered file exists\\n    if not os.path.exists(filtered_file_path):\\n        print(f\"Filtered file {filtered_file_path} does not exist. Skipping.\")\\n        continue\\n    \\n    # Read the filtered CSV file\\n    df = pd.read_csv(filtered_file_path, encoding=\\'ISO-8859-1\\')  # Adjust encoding if necessary\\n    \\n    # Convert \\'count_date\\' to datetime and extract the date part\\n    df[\\'count_date\\'] = pd.to_datetime(df[\\'count_date\\'], errors=\\'coerce\\')\\n    df[\\'date\\'] = df[\\'count_date\\'].dt.date\\n    \\n    # Group by day and calculate the mean for specific columns\\n    columns_to_interpolate = [\\n        \\'pedal_cycles\\', \\'two_wheeled_motor_vehicles\\', \\'cars_and_taxis\\', \\n        \\'buses_and_coaches\\', \\'lgvs\\', \\'hgvs_2_rigid_axle\\', \\'hgvs_3_rigid_axle\\', \\n        \\'hgvs_4_or_more_rigid_axle\\', \\'hgvs_3_or_4_articulated_axle\\', \\n        \\'hgvs_5_articulated_axle\\', \\'hgvs_6_articulated_axle\\', \\'all_hgvs\\', \\n        \\'all_motor_vehicles\\'\\n    ]\\n    daily_df = df.groupby(\\'date\\')[columns_to_interpolate].mean().reset_index()\\n    \\n    # Create a complete date range\\n    full_date_range = pd.date_range(start=daily_df[\\'date\\'].min(), end=daily_df[\\'date\\'].max())\\n    \\n    # Reindex to have a continuous date range\\n    daily_df = daily_df.set_index(\\'date\\').reindex(full_date_range).reset_index()\\n    daily_df.rename(columns={\\'index\\': \\'date\\'}, inplace=True)\\n    \\n    # Interpolate missing values using spline interpolation for specified columns\\n    for column in columns_to_interpolate:\\n        mask = np.isfinite(daily_df[column])\\n        x = np.arange(len(daily_df))\\n        spline = UnivariateSpline(x[mask], daily_df.loc[mask, column], s=0)\\n        daily_df[column] = spline(x)\\n        daily_df[column] = daily_df[column].round(2)  # Round to 2 decimal places\\n    \\n    # Save the processed data to a new CSV file\\n    processed_file_path = filtered_file_path.replace(\\'_filtered.csv\\', \\'_processed.csv\\')\\n    daily_df.to_csv(processed_file_path, index=False)\\n    \\n    print(f\"Processed data saved to {processed_file_path}\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Step 2: Process the filtered files\n",
    "filtered_file_paths = [file.replace('.csv', '_filtered.csv') for file in file_paths]\n",
    "\n",
    "for filtered_file_path in filtered_file_paths:\n",
    "    # Check if filtered file exists\n",
    "    if not os.path.exists(filtered_file_path):\n",
    "        print(f\"Filtered file {filtered_file_path} does not exist. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Read the filtered CSV file\n",
    "    df = pd.read_csv(filtered_file_path, encoding='ISO-8859-1')  # Adjust encoding if necessary\n",
    "    \n",
    "    # Convert 'count_date' to datetime and extract the date part\n",
    "    df['count_date'] = pd.to_datetime(df['count_date'], errors='coerce')\n",
    "    df['date'] = df['count_date'].dt.date\n",
    "    \n",
    "    # Group by day and calculate the mean for specific columns\n",
    "    columns_to_interpolate = [\n",
    "        'pedal_cycles', 'two_wheeled_motor_vehicles', 'cars_and_taxis', \n",
    "        'buses_and_coaches', 'lgvs', 'hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', \n",
    "        'hgvs_4_or_more_rigid_axle', 'hgvs_3_or_4_articulated_axle', \n",
    "        'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle', 'all_hgvs', \n",
    "        'all_motor_vehicles'\n",
    "    ]\n",
    "    daily_df = df.groupby('date')[columns_to_interpolate].mean().reset_index()\n",
    "    \n",
    "    # Create a complete date range\n",
    "    full_date_range = pd.date_range(start=daily_df['date'].min(), end=daily_df['date'].max())\n",
    "    \n",
    "    # Reindex to have a continuous date range\n",
    "    daily_df = daily_df.set_index('date').reindex(full_date_range).reset_index()\n",
    "    daily_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "    \n",
    "    # Interpolate missing values using spline interpolation for specified columns\n",
    "    for column in columns_to_interpolate:\n",
    "        mask = np.isfinite(daily_df[column])\n",
    "        x = np.arange(len(daily_df))\n",
    "        spline = UnivariateSpline(x[mask], daily_df.loc[mask, column], s=0)\n",
    "        daily_df[column] = spline(x)\n",
    "        daily_df[column] = daily_df[column].round(2)  # Round to 2 decimal places\n",
    "    \n",
    "    # Save the processed data to a new CSV file\n",
    "    processed_file_path = filtered_file_path.replace('_filtered.csv', '_processed.csv')\n",
    "    daily_df.to_csv(processed_file_path, index=False)\n",
    "    \n",
    "    print(f\"Processed data saved to {processed_file_path}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
