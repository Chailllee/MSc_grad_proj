{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.linalg import solve\n",
    "from scipy.optimize import curve_fit\n",
    "# from geopy.distance import geodesic\n",
    "from pykrige.ok import OrdinaryKriging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate_semivariogram 计算半方差函数\n",
    "def calculate_semivariogram(data):\n",
    "    num_points = len(data)\n",
    "    semivariances = []\n",
    "\n",
    "    for i in range(num_points):\n",
    "        for j in range(i + 1, num_points):\n",
    "            dist = np.linalg.norm([data['Latitude'].iloc[i] - data['Latitude'].iloc[j],\n",
    "                                   data['Longitude'].iloc[i] - data['Longitude'].iloc[j]])\n",
    "            squared_diff = (data['Value'].iloc[i] - data['Value'].iloc[j]) ** 2\n",
    "            semivariances.append((dist, squared_diff))\n",
    "\n",
    "    unique_distances = sorted(set([item[0] for item in semivariances]))\n",
    "    avg_semivariances = []\n",
    "    for dist in unique_distances:\n",
    "        squared_diffs = [item[1] for item in semivariances if item[0] == dist]\n",
    "        avg_semivariances.append((dist, np.mean(squared_diffs) / 2.0))\n",
    "\n",
    "    return np.array(avg_semivariances)\n",
    "\n",
    "# calculate_kriging_weights 计算克里金权重\n",
    "def calculate_kriging_weights(semivariogram, distances, n, nugget=1e-10):\n",
    "    A = np.zeros((n + 1, n + 1))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                A[i, j] = semivariogram[0][1] + nugget\n",
    "            else:\n",
    "                dist = int(distances[0, j])\n",
    "                A[i, j] = semivariogram[dist][1] if dist < len(semivariogram) else semivariogram[-1][1]\n",
    "\n",
    "    A[-1, :-1] = 1\n",
    "    A[:-1, -1] = 1\n",
    "\n",
    "    b = np.zeros(n + 1)\n",
    "    for i in range(n):\n",
    "        dist = int(distances[0, i])\n",
    "        b[i] = semivariogram[dist][1] if dist < len(semivariogram) else semivariogram[-1][1]\n",
    "\n",
    "    weights = solve(A, b)\n",
    "    return weights[:-1]\n",
    "\n",
    "\n",
    "def adjust_weights(weights, wind_speed, wind_dir, sensor_directions, max_wind_speed):\n",
    "    adjustments = 1 + (wind_speed * np.cos(np.radians(wind_dir - sensor_directions))) / max_wind_speed\n",
    "    adjusted_weights = np.clip(weights * adjustments, 0, None)  # Ensure weights are non-negative\n",
    "    return adjusted_weights\n",
    "\n",
    "def normalize_weights(weights):\n",
    "    total_weight = np.sum(weights)\n",
    "    if total_weight == 0:\n",
    "        return np.zeros_like(weights)\n",
    "    return weights / total_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_20936\\1101129135.py:8: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  wind_data_path = f'{meteostat_folder}\\meteostat{year}.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# interpolate 插值估算\n",
    "def interpolate(data, weights):\n",
    "    return np.sum(weights * data['Value'].values)\n",
    "\n",
    "\n",
    "# 修改 get_wind_data 函数\n",
    "def get_wind_data(date, year):\n",
    "    wind_data_path = f'{meteostat_folder}\\meteostat{year}.csv'\n",
    "    wind_data_df = pd.read_csv(wind_data_path, encoding='utf-8-sig')  # 处理BOM\n",
    "    wind_data_df.columns = wind_data_df.columns.str.strip()\n",
    "\n",
    "    # # Debugging: Print column names to verify\n",
    "    # print(\"Column names in wind_data_df:\", wind_data_df.columns)\n",
    "\n",
    "    # 确保日期格式一致\n",
    "    wind_data_df['date'] = pd.to_datetime(wind_data_df['date'], format='%Y-%m-%d').dt.date\n",
    "    date = pd.to_datetime(date, format='%Y-%m-%d').date()  # 确保date参数也被转换成相同格式\n",
    "\n",
    "    wind_info = wind_data_df[wind_data_df['date'] == date]\n",
    "    if not wind_info.empty:\n",
    "        return wind_info['wspd'].values[0], wind_info['wdir'].values[0]\n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "\n",
    "def interpolate_and_clip(df, column, method='linear', order=2):\n",
    "    # 获取原始列的最小值和最大值\n",
    "    original_min = df[column].min()\n",
    "    original_max = df[column].max()\n",
    "    \n",
    "    # 进行插值\n",
    "    if method == 'polynomial':\n",
    "        interpolated_values = df[column].interpolate(method=method, order=order)\n",
    "    else:\n",
    "        interpolated_values = df[column].interpolate(method=method)\n",
    "    \n",
    "    # 确保插值后的值在原来值的 {min, max} 范围之间\n",
    "    interpolated_values = interpolated_values.clip(lower=original_min, upper=original_max).round(2)\n",
    "    \n",
    "    # 仅更新空值部分\n",
    "    return df[column].combine_first(interpolated_values)\n",
    "\n",
    "# 自定义的多项式插值函数\n",
    "def polynomial_interpolation(series, order=2):\n",
    "    return series.interpolate(method='polynomial', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(base_folder, region):\n",
    "    # 构建文件路径\n",
    "    region_folder = os.path.join(base_folder, region)\n",
    "    no_path = os.path.join(region_folder, f'{region}-Nitrogen Dioxide (ug m-3).csv')\n",
    "    coords_path = os.path.join(base_folder, 'coords_londonair.csv')\n",
    "    \n",
    "    # 读取数据\n",
    "    no_df = pd.read_csv(no_path)\n",
    "    coords_df = pd.read_csv(coords_path)\n",
    "    \n",
    "    # 数据预处理\n",
    "    no_df['ReadingDateTime'] = pd.to_datetime(no_df['ReadingDateTime'], format='%d/%m/%Y %H:%M')\n",
    "    no_df['Date'] = no_df['ReadingDateTime'].dt.date\n",
    "    coords_df[['Latitude', 'Longitude']] = coords_df['Latitude & Longitude'].str.split(', ', expand=True).astype(float)\n",
    "    \n",
    "    # 合并坐标信息\n",
    "    no_df = pd.merge(no_df, coords_df[['Site', 'Latitude', 'Longitude']], on='Site', how='left')\n",
    "    \n",
    "    # 只处理排在最前面的第一种“Site”的Value值\n",
    "    first_site = no_df['Site'].unique()[0]\n",
    "    no_df = no_df[no_df['Site'] == first_site]\n",
    "    \n",
    "    # 检查并处理 NaN 值\n",
    "    if no_df.isnull().values.any():\n",
    "        print(f\"NaN values found in {region} data before interpolation\")\n",
    "    \n",
    "    # 对于第一个site的任何空缺时间的值，根据其他年份同日期的数据规律进行填充\n",
    "    no_df.set_index('ReadingDateTime', inplace=True)\n",
    "    monthly_daily_mean = no_df.groupby([no_df.index.month, no_df.index.day])['Value'].transform('mean')\n",
    "    no_df['Value'].fillna(monthly_daily_mean, inplace=True)\n",
    "    no_df.reset_index(inplace=True)\n",
    "    \n",
    "    # 检查插值结果\n",
    "    if no_df['Value'].isnull().values.any():\n",
    "        print(f\"NaN values found in {region} data after interpolation\")\n",
    "    \n",
    "    return no_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义地区列表\n",
    "regions = [\n",
    "    'Camden', 'City of London', 'Islington', \n",
    "    'Kensington and Chelsea', 'Lambeth', \n",
    "    'Southwark', 'Westminster'\n",
    "]\n",
    "\n",
    "# 定义文件夹路径\n",
    "base_folder = r'D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\LondonAir'\n",
    "meteostat_folder = r'D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\meteostat\\\\'\n",
    "NO_weighted_output_folder = r'D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data_output\\NO_weighted\\\\'\n",
    "\n",
    "# 确保输出文件夹存在\n",
    "if not os.path.exists(NO_weighted_output_folder):\n",
    "    os.makedirs(NO_weighted_output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing region: Camden\n",
      "NaN values found in Camden data before interpolation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_20936\\2181997684.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  no_df['Value'].fillna(monthly_daily_mean, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted data saved to D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data_output\\NO_weighted\\\\Camden-NO2_weighted.csv\n",
      "Processing region: City of London\n",
      "NaN values found in City of London data before interpolation\n",
      "NaN values found in City of London data after interpolation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_20936\\2181997684.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  no_df['Value'].fillna(monthly_daily_mean, inplace=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN or inf values found in distances for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     distances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(distances, nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, posinf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, neginf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m kriging_weights \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_kriging_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43msemivariogram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdaily_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m wind_speed, wind_dir \u001b[38;5;241m=\u001b[39m get_wind_data(date, date\u001b[38;5;241m.\u001b[39myear)\n\u001b[0;32m     25\u001b[0m sensor_directions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marctan2(daily_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m daily_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(), daily_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m daily_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m180\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpi\n",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m, in \u001b[0;36mcalculate_kriging_weights\u001b[1;34m(semivariogram, distances, n, nugget)\u001b[0m\n\u001b[0;32m     38\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(distances[\u001b[38;5;241m0\u001b[39m, i])\n\u001b[0;32m     39\u001b[0m     b[i] \u001b[38;5;241m=\u001b[39m semivariogram[dist][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(semivariogram) \u001b[38;5;28;01melse\u001b[39;00m semivariogram[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 41\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weights[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\scipy\\linalg\\_basic.py:148\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(a, b, lower, overwrite_a, overwrite_b, check_finite, assume_a, transposed)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Flags for 1-D or N-D right-hand side\u001b[39;00m\n\u001b[0;32m    146\u001b[0m b_is_1D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m a1 \u001b[38;5;241m=\u001b[39m atleast_2d(\u001b[43m_asarray_validated\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    149\u001b[0m b1 \u001b[38;5;241m=\u001b[39m atleast_1d(_asarray_validated(b, check_finite\u001b[38;5;241m=\u001b[39mcheck_finite))\n\u001b[0;32m    150\u001b[0m n \u001b[38;5;241m=\u001b[39m a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\scipy\\_lib\\_util.py:321\u001b[0m, in \u001b[0;36m_asarray_validated\u001b[1;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasked arrays are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    320\u001b[0m toarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray_chkfinite \u001b[38;5;28;01mif\u001b[39;00m check_finite \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray\n\u001b[1;32m--> 321\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m objects_ok:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\numpy\\lib\\function_base.py:630\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    628\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m typecodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllFloat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(a)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "\n",
    "# 遍历每个地区\n",
    "for region in regions:\n",
    "    print(f\"Processing region: {region}\")\n",
    "    \n",
    "    no_df = preprocess_data(base_folder, region)\n",
    "    \n",
    "    # Kriging 计算\n",
    "    weighted_values = []\n",
    "    dates = no_df['Date'].unique()\n",
    "    \n",
    "    for date in dates:\n",
    "        daily_data = no_df[no_df['Date'] == date]\n",
    "        if len(daily_data) > 1:\n",
    "            semivariogram = calculate_semivariogram(daily_data)\n",
    "            distances = cdist(daily_data[['Latitude', 'Longitude']], daily_data[['Latitude', 'Longitude']], metric='euclidean')\n",
    "            \n",
    "            # 检查并处理 NaN 和 inf 值\n",
    "            if np.isnan(distances).any() or np.isinf(distances).any():\n",
    "                print(f\"NaN or inf values found in distances for {region} on {date}\")\n",
    "                distances = np.nan_to_num(distances, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            kriging_weights = calculate_kriging_weights(semivariogram, distances, len(daily_data))\n",
    "            \n",
    "            wind_speed, wind_dir = get_wind_data(date, date.year)\n",
    "            sensor_directions = np.arctan2(daily_data['Longitude'] - daily_data['Longitude'].mean(), daily_data['Latitude'] - daily_data['Latitude'].mean()) * 180 / np.pi\n",
    "            meteostat_path = f'{meteostat_folder}\\\\meteostat{date.year}.csv'\n",
    "            meteostat_df = pd.read_csv(meteostat_path)\n",
    "            max_wind_speed = meteostat_df['wspd'].max()\n",
    "            \n",
    "            adjusted_weights = adjust_weights(kriging_weights, wind_speed, wind_dir, sensor_directions, max_wind_speed)\n",
    "            normalized_weights = normalize_weights(adjusted_weights)\n",
    "            \n",
    "            weighted_value = interpolate(daily_data, normalized_weights)\n",
    "            weighted_value = max(0, weighted_value)  # 确保没有负值\n",
    "            weighted_values.append({'Date': date, 'NO2_weighted_value(ug m-3)': weighted_value})\n",
    "        else:\n",
    "            weighted_values.append({'Date': date, 'NO2_weighted_value(ug m-3)': daily_data['Value'].values[0]})\n",
    "    \n",
    "    # 保存加权后的值\n",
    "    weighted_df = pd.DataFrame(weighted_values)\n",
    "    output_path = os.path.join(NO_weighted_output_folder, f'{region}-NO2_weighted.csv')\n",
    "    weighted_df.to_csv(output_path, index=False)\n",
    "    print(f\"Weighted data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
