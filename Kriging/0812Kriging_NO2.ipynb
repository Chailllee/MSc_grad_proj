{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.linalg import solve\n",
    "from scipy.optimize import curve_fit\n",
    "# from geopy.distance import geodesic\n",
    "from pykrige.ok import OrdinaryKriging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate_semivariogram 计算半方差函数\n",
    "def calculate_semivariogram(data):\n",
    "    num_points = len(data)\n",
    "    semivariances = []\n",
    "\n",
    "    for i in range(num_points):\n",
    "        for j in range(i + 1, num_points):\n",
    "            dist = np.linalg.norm([data['Latitude'].iloc[i] - data['Latitude'].iloc[j],\n",
    "                                   data['Longitude'].iloc[i] - data['Longitude'].iloc[j]])\n",
    "            squared_diff = (data['Value'].iloc[i] - data['Value'].iloc[j]) ** 2\n",
    "            semivariances.append((dist, squared_diff))\n",
    "\n",
    "    unique_distances = sorted(set([item[0] for item in semivariances]))\n",
    "    avg_semivariances = []\n",
    "    for dist in unique_distances:\n",
    "        squared_diffs = [item[1] for item in semivariances if item[0] == dist]\n",
    "        avg_semivariances.append((dist, np.mean(squared_diffs) / 2.0))\n",
    "\n",
    "    return np.array(avg_semivariances)\n",
    "\n",
    "# calculate_kriging_weights 计算克里金权重\n",
    "def calculate_kriging_weights(semivariogram, distances, n, nugget=1e-10):\n",
    "    A = np.zeros((n + 1, n + 1))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                A[i, j] = semivariogram[0][1] + nugget\n",
    "            else:\n",
    "                dist = int(distances[0, j])\n",
    "                A[i, j] = semivariogram[dist][1] if dist < len(semivariogram) else semivariogram[-1][1]\n",
    "\n",
    "    A[-1, :-1] = 1\n",
    "    A[:-1, -1] = 1\n",
    "\n",
    "    b = np.zeros(n + 1)\n",
    "    for i in range(n):\n",
    "        dist = int(distances[0, i])\n",
    "        b[i] = semivariogram[dist][1] if dist < len(semivariogram) else semivariogram[-1][1]\n",
    "\n",
    "    weights = solve(A, b)\n",
    "    return weights[:-1]\n",
    "\n",
    "\n",
    "def adjust_weights(weights, wind_speed, wind_dir, sensor_directions, max_wind_speed):\n",
    "    adjustments = 1 + (wind_speed * np.cos(np.radians(wind_dir - sensor_directions))) / max_wind_speed\n",
    "    adjusted_weights = np.clip(weights * adjustments, 0, None)  # Ensure weights are non-negative\n",
    "    return adjusted_weights\n",
    "\n",
    "def normalize_weights(weights):\n",
    "    total_weight = np.sum(weights)\n",
    "    if total_weight == 0:\n",
    "        return np.zeros_like(weights)\n",
    "    return weights / total_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_20936\\1101129135.py:8: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  wind_data_path = f'{meteostat_folder}\\meteostat{year}.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# interpolate 插值估算\n",
    "def interpolate(data, weights):\n",
    "    return np.sum(weights * data['Value'].values)\n",
    "\n",
    "\n",
    "# 修改 get_wind_data 函数\n",
    "def get_wind_data(date, year):\n",
    "    wind_data_path = f'{meteostat_folder}\\meteostat{year}.csv'\n",
    "    wind_data_df = pd.read_csv(wind_data_path, encoding='utf-8-sig')  # 处理BOM\n",
    "    wind_data_df.columns = wind_data_df.columns.str.strip()\n",
    "\n",
    "    # # Debugging: Print column names to verify\n",
    "    # print(\"Column names in wind_data_df:\", wind_data_df.columns)\n",
    "\n",
    "    # 确保日期格式一致\n",
    "    wind_data_df['date'] = pd.to_datetime(wind_data_df['date'], format='%Y-%m-%d').dt.date\n",
    "    date = pd.to_datetime(date, format='%Y-%m-%d').date()  # 确保date参数也被转换成相同格式\n",
    "\n",
    "    wind_info = wind_data_df[wind_data_df['date'] == date]\n",
    "    if not wind_info.empty:\n",
    "        return wind_info['wspd'].values[0], wind_info['wdir'].values[0]\n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "\n",
    "def interpolate_and_clip(df, column, method='linear', order=2):\n",
    "    # 获取原始列的最小值和最大值\n",
    "    original_min = df[column].min()\n",
    "    original_max = df[column].max()\n",
    "    \n",
    "    # 进行插值\n",
    "    if method == 'polynomial':\n",
    "        interpolated_values = df[column].interpolate(method=method, order=order)\n",
    "    else:\n",
    "        interpolated_values = df[column].interpolate(method=method)\n",
    "    \n",
    "    # 确保插值后的值在原来值的 {min, max} 范围之间\n",
    "    interpolated_values = interpolated_values.clip(lower=original_min, upper=original_max).round(2)\n",
    "    \n",
    "    # 仅更新空值部分\n",
    "    return df[column].combine_first(interpolated_values)\n",
    "\n",
    "# 自定义的多项式插值函数\n",
    "def polynomial_interpolation(series, order=2):\n",
    "    return series.interpolate(method='polynomial', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(base_folder, region):\n",
    "    # 构建文件路径\n",
    "    region_folder = os.path.join(base_folder, region)\n",
    "    no_path = os.path.join(region_folder, f'{region}-Nitrogen Dioxide (ug m-3).csv')\n",
    "    coords_path = os.path.join(base_folder, 'coords_londonair.csv')\n",
    "    \n",
    "    # 读取数据\n",
    "    no_df = pd.read_csv(no_path)\n",
    "    coords_df = pd.read_csv(coords_path)\n",
    "    \n",
    "    # 数据预处理\n",
    "    no_df['ReadingDateTime'] = pd.to_datetime(no_df['ReadingDateTime'], format='%d/%m/%Y %H:%M')\n",
    "    no_df['Date'] = no_df['ReadingDateTime'].dt.date\n",
    "    coords_df[['Latitude', 'Longitude']] = coords_df['Latitude & Longitude'].str.split(', ', expand=True).astype(float)\n",
    "    \n",
    "    # 合并坐标信息\n",
    "    no_df = pd.merge(no_df, coords_df[['Site', 'Latitude', 'Longitude']], on='Site', how='left')\n",
    "    \n",
    "    # 只处理排在最前面的第一种“Site”的Value值\n",
    "    first_site = no_df['Site'].unique()[0]\n",
    "    no_df = no_df[no_df['Site'] == first_site]\n",
    "    \n",
    "    # 检查并处理 NaN 值\n",
    "    if no_df.isnull().values.any():\n",
    "        print(f\"NaN values found in {region} data before interpolation\")\n",
    "    \n",
    "    # 对于第一个site的任何空缺时间的值，根据其他年份同日期的数据规律进行填充\n",
    "    no_df.set_index('ReadingDateTime', inplace=True)\n",
    "    monthly_daily_mean = no_df.groupby([no_df.index.month, no_df.index.day])['Value'].transform('mean')\n",
    "    no_df['Value'].fillna(monthly_daily_mean, inplace=True)\n",
    "    no_df.reset_index(inplace=True)\n",
    "    \n",
    "    # 检查插值结果\n",
    "    if no_df['Value'].isnull().values.any():\n",
    "        print(f\"NaN values found in {region} data after interpolation\")\n",
    "    \n",
    "    return no_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breathe London data preprocessing\n",
    "def preprocess_data(base_folder, breathe_folder, region):\n",
    "    # 构建文件路径\n",
    "    region_folder = os.path.join(base_folder, region)\n",
    "    no_path = os.path.join(region_folder, f'{region}-Nitrogen Dioxide (ug m-3).csv')\n",
    "    coords_path = os.path.join(base_folder, 'coords_londonair.csv')\n",
    "    \n",
    "    # 读取数据\n",
    "    no_df = pd.read_csv(no_path)\n",
    "    coords_df = pd.read_csv(coords_path)\n",
    "    \n",
    "    # 数据预处理\n",
    "    no_df['ReadingDateTime'] = pd.to_datetime(no_df['ReadingDateTime'], format='%d/%m/%Y %H:%M')\n",
    "    no_df['Date'] = no_df['ReadingDateTime'].dt.date\n",
    "    coords_df[['Latitude', 'Longitude']] = coords_df['Latitude & Longitude'].str.split(', ', expand=True).astype(float)\n",
    "    \n",
    "    # 合并坐标信息\n",
    "    no_df = pd.merge(no_df, coords_df[['Site', 'Latitude', 'Longitude']], on='Site', how='left')\n",
    "    \n",
    "    # 只处理排在最前面的第一种“Site”的Value值\n",
    "    first_site = no_df['Site'].unique()[0]\n",
    "    no_df = no_df[no_df['Site'] == first_site]\n",
    "    \n",
    "    # 检查并处理 NaN 值\n",
    "    if no_df.isnull().values.any():\n",
    "        print(f\"NaN values found in {region} data before interpolation\")\n",
    "    \n",
    "    # 对于第一个site的任何空缺时间的值，根据其他年份同日期的数据规律进行填充\n",
    "    no_df.set_index('ReadingDateTime', inplace=True)\n",
    "    monthly_daily_mean = no_df.groupby([no_df.index.month, no_df.index.day])['Value'].transform('mean')\n",
    "    no_df['Value'].fillna(monthly_daily_mean, inplace=True)\n",
    "    no_df.reset_index(inplace=True)\n",
    "    \n",
    "    # 如果 2021 年之后的数据有缺失，则使用数据库 BreatheLondon2 对对应地区的 NO2 记录进行补充\n",
    "    if no_df[no_df['ReadingDateTime'] > '27/01/2021']['Value'].isnull().values.any():\n",
    "        breathe_region_folder = os.path.join(breathe_folder, region)\n",
    "        breathe_files = [f for f in os.listdir(breathe_region_folder) if f.endswith('_NO2.csv')]\n",
    "        \n",
    "        for file in breathe_files:\n",
    "            try:\n",
    "                breathe_no2_df = pd.read_csv(os.path.join(breathe_region_folder, file))\n",
    "                \n",
    "                # 统一日期格式\n",
    "                breathe_no2_df['Date'] = pd.to_datetime(breathe_no2_df['Category'], format='%Y-%m-%d')\n",
    "                breathe_no2_df.rename(columns={'NO2_mean': 'Value'}, inplace=True)\n",
    "                \n",
    "                # 合并数据\n",
    "                no_df = pd.merge(no_df, breathe_no2_df[['ReadingDateTime', 'Value']], on='ReadingDateTime', how='left', suffixes=('', '_breathe'))\n",
    "                no_df['Value'] = no_df['Value'].combine_first(no_df['Value_breathe'])\n",
    "                no_df.drop(columns=['Value_breathe'], inplace=True)\n",
    "            \n",
    "            except ValueError as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error processing file {file}: {e}\")\n",
    "    # 检查插值结果\n",
    "    if no_df['Value'].isnull().values.any():\n",
    "        print(f\"NaN values found in {region} data after interpolation\")\n",
    "    \n",
    "    return no_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: invalid escape sequence '\\F'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\F'\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_20936\\1358017423.py:13: SyntaxWarning: invalid escape sequence '\\F'\n",
      "  breathe_folder = 'D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\\\AirQuality\\\\BreatheLondon2'\n"
     ]
    }
   ],
   "source": [
    "# 定义地区列表\n",
    "regions = [\n",
    "    'Camden', 'City of London', 'Islington', \n",
    "    'Kensington and Chelsea', 'Lambeth', \n",
    "    'Southwark', 'Westminster'\n",
    "]\n",
    "\n",
    "# 定义文件夹路径\n",
    "base_folder = r'D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\LondonAir'\n",
    "meteostat_folder = r'D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\meteostat\\\\'\n",
    "NO_weighted_output_folder = r'D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data_output\\NO_weighted\\\\'\n",
    "\n",
    "breathe_folder = 'D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\\\AirQuality\\\\BreatheLondon2'\n",
    "\n",
    "# 确保输出文件夹存在\n",
    "if not os.path.exists(NO_weighted_output_folder):\n",
    "    os.makedirs(NO_weighted_output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing region: Camden\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "embedded null character",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m region \u001b[38;5;129;01min\u001b[39;00m regions:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing region: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     no_df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreathe_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Kriging 计算\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     weighted_values \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[23], line 11\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(base_folder, breathe_folder, region)\u001b[0m\n\u001b[0;32m      9\u001b[0m no_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(no_path)\n\u001b[0;32m     10\u001b[0m coords_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(coords_path)\n\u001b[1;32m---> 11\u001b[0m breathe_coords_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbreathe_coords_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 数据预处理\u001b[39;00m\n\u001b[0;32m     14\u001b[0m no_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReadingDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(no_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReadingDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mValueError\u001b[0m: embedded null character"
     ]
    }
   ],
   "source": [
    "\n",
    "# 遍历每个地区\n",
    "for region in regions:\n",
    "    print(f\"Processing region: {region}\")\n",
    "    \n",
    "    no_df = preprocess_data(base_folder, region)\n",
    "    \n",
    "    # Kriging 计算\n",
    "    weighted_values = []\n",
    "    dates = no_df['Date'].unique()\n",
    "    \n",
    "    for date in dates:\n",
    "        daily_data = no_df[no_df['Date'] == date]\n",
    "        if len(daily_data) > 1:\n",
    "            semivariogram = calculate_semivariogram(daily_data)\n",
    "            distances = cdist(daily_data[['Latitude', 'Longitude']], daily_data[['Latitude', 'Longitude']], metric='euclidean')\n",
    "            \n",
    "            # 检查并处理 NaN 和 inf 值\n",
    "            if np.isnan(distances).any() or np.isinf(distances).any():\n",
    "                print(f\"NaN or inf values found in distances for {region} on {date}\")\n",
    "                distances = np.nan_to_num(distances, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            kriging_weights = calculate_kriging_weights(semivariogram, distances, len(daily_data))\n",
    "            \n",
    "            wind_speed, wind_dir = get_wind_data(date, date.year)\n",
    "            sensor_directions = np.arctan2(daily_data['Longitude'] - daily_data['Longitude'].mean(), daily_data['Latitude'] - daily_data['Latitude'].mean()) * 180 / np.pi\n",
    "            meteostat_path = f'{meteostat_folder}\\\\meteostat{date.year}.csv'\n",
    "            meteostat_df = pd.read_csv(meteostat_path)\n",
    "            max_wind_speed = meteostat_df['wspd'].max()\n",
    "            \n",
    "            adjusted_weights = adjust_weights(kriging_weights, wind_speed, wind_dir, sensor_directions, max_wind_speed)\n",
    "            normalized_weights = normalize_weights(adjusted_weights)\n",
    "            \n",
    "            weighted_value = interpolate(daily_data, normalized_weights)\n",
    "            weighted_value = max(0, weighted_value)  # 确保没有负值\n",
    "            weighted_values.append({'Date': date, 'NO2_weighted_value(ug m-3)': weighted_value})\n",
    "        else:\n",
    "            weighted_values.append({'Date': date, 'NO2_weighted_value(ug m-3)': daily_data['Value'].values[0]})\n",
    "    \n",
    "    # 保存加权后的值\n",
    "    weighted_df = pd.DataFrame(weighted_values)\n",
    "    output_path = os.path.join(NO_weighted_output_folder, f'{region}-NO2_weighted.csv')\n",
    "    weighted_df.to_csv(output_path, index=False)\n",
    "    print(f\"Weighted data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
