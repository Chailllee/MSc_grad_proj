{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.linalg import solve\n",
    "from scipy.optimize import curve_fit\n",
    "# from geopy.distance import geodesic\n",
    "from pykrige.ok import OrdinaryKriging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 读取数据\n",
    "camden_no_path = r'D:\\File_auto\\\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\LondonAir\\Camden\\Camden-Nitric Oxide (ug m-3).csv'\n",
    "coords_path = r'D:\\File_auto\\\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\LondonAir\\coords_londonair.csv'\n",
    "meteostat_folder = r'D:\\File_auto\\\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\meteostat'\n",
    "\n",
    "weighted_no_output_folder = r'D:\\File_auto\\\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data_output\\NO_weighted'\n",
    "\n",
    "camden_no_df = pd.read_csv(camden_no_path)\n",
    "coords_df = pd.read_csv(coords_path)\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "camden_no_df['ReadingDateTime'] = pd.to_datetime(camden_no_df['ReadingDateTime'], format='%d/%m/%Y %H:%M')\n",
    "camden_no_df['Date'] = camden_no_df['ReadingDateTime'].dt.date\n",
    "coords_df[['Latitude', 'Longitude']] = coords_df['Latitude & Longitude'].str.split(', ', expand=True).astype(float)\n",
    "\n",
    "# 合并坐标信息\n",
    "camden_no_df = pd.merge(camden_no_df, coords_df[['Site', 'Latitude', 'Longitude']], on='Site', how='left')\n",
    "\n",
    "camden_no_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure all components of your Kriging process are robust to data issues and correctly implemented.\n",
    "# 对 \"Value\" 列进行插值并裁剪\n",
    "camden_no_df['Value'] = interpolate_and_clip(camden_no_df, 'Value', method='linear')\n",
    "\n",
    "# Adjustments in your main Kriging loop\n",
    "weighted_values = []\n",
    "dates = camden_no_df['Date'].unique()\n",
    "\n",
    "for date in dates:\n",
    "    daily_data = camden_no_df[camden_no_df['Date'] == date]\n",
    "    if len(daily_data) > 1:\n",
    "        semivariogram = calculate_semivariogram(daily_data)  \n",
    "        # distances = calculate_distances(daily_data[['Latitude', 'Longitude']].values)\n",
    "        distances = cdist(daily_data[['Latitude', 'Longitude']], daily_data[['Latitude', 'Longitude']], metric='euclidean')\n",
    "        # params = fit_semivariogram(daily_data, 10000) # Define an appropriate max_distance\n",
    "        kriging_weights = calculate_kriging_weights(semivariogram, distances, len(daily_data))\n",
    "\n",
    "        wind_speed, wind_dir = get_wind_data(date, date.year)\n",
    "        sensor_directions = np.arctan2(daily_data['Longitude'] - daily_data['Longitude'].mean(), daily_data['Latitude'] - daily_data['Latitude'].mean()) * 180 / np.pi\n",
    "        # max_wind_speed = wind_speed\n",
    "        # 读取对应年份的 meteostat 数据\n",
    "        meteostat_path = f'{meteostat_folder}\\meteostat{date.year}.csv'\n",
    "        meteostat_df = pd.read_csv(meteostat_path)\n",
    "        # 获取该年份中 \"wspd\" 列的最大值\n",
    "        max_wind_speed = meteostat_df['wspd'].max()\n",
    "\n",
    "        adjusted_weights = adjust_weights(kriging_weights, wind_speed, wind_dir, sensor_directions, max_wind_speed)\n",
    "        normalized_weights = normalize_weights(adjusted_weights)\n",
    "\n",
    "        weighted_value = interpolate(daily_data, normalized_weights)\n",
    "        weighted_value = max(0, weighted_value)  # Ensure no negative values\n",
    "        weighted_values.append({'Date': date, 'NO_weighted_value(ug m-3)': weighted_value})\n",
    "    else:\n",
    "        weighted_values.append({'Date': date, 'NO_weighted_value(ug m-3)': daily_data['Value'].values[0]})\n",
    "\n",
    "# Save the weighted values\n",
    "weighted_df = pd.DataFrame(weighted_values)\n",
    "output_path = os.path.join(weighted_no_output_folder, 'Camden-NO_weighted.csv')\n",
    "weighted_df.to_csv(output_path, index=False)\n",
    "print(f\"Weighted data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate_semivariogram 计算半方差函数\n",
    "def calculate_semivariogram(data):\n",
    "    num_points = len(data)\n",
    "    semivariances = []\n",
    "\n",
    "    for i in range(num_points):\n",
    "        for j in range(i + 1, num_points):\n",
    "            dist = np.linalg.norm([data['Latitude'].iloc[i] - data['Latitude'].iloc[j],\n",
    "                                   data['Longitude'].iloc[i] - data['Longitude'].iloc[j]])\n",
    "            squared_diff = (data['Value'].iloc[i] - data['Value'].iloc[j]) ** 2\n",
    "            semivariances.append((dist, squared_diff))\n",
    "\n",
    "    unique_distances = sorted(set([item[0] for item in semivariances]))\n",
    "    avg_semivariances = []\n",
    "    for dist in unique_distances:\n",
    "        squared_diffs = [item[1] for item in semivariances if item[0] == dist]\n",
    "        avg_semivariances.append((dist, np.mean(squared_diffs) / 2.0))\n",
    "\n",
    "    return np.array(avg_semivariances)\n",
    "\n",
    "# calculate_kriging_weights 计算克里金权重\n",
    "def calculate_kriging_weights(semivariogram, distances, n, nugget=1e-10):\n",
    "    A = np.zeros((n + 1, n + 1))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                A[i, j] = semivariogram[0][1] + nugget\n",
    "            else:\n",
    "                dist = int(distances[0, j])\n",
    "                A[i, j] = semivariogram[dist][1] if dist < len(semivariogram) else semivariogram[-1][1]\n",
    "\n",
    "    A[-1, :-1] = 1\n",
    "    A[:-1, -1] = 1\n",
    "\n",
    "    b = np.zeros(n + 1)\n",
    "    for i in range(n):\n",
    "        dist = int(distances[0, i])\n",
    "        b[i] = semivariogram[dist][1] if dist < len(semivariogram) else semivariogram[-1][1]\n",
    "\n",
    "    weights = solve(A, b)\n",
    "    return weights[:-1]\n",
    "\n",
    "\n",
    "def adjust_weights(weights, wind_speed, wind_dir, sensor_directions, max_wind_speed):\n",
    "    adjustments = 1 + (wind_speed * np.cos(np.radians(wind_dir - sensor_directions))) / max_wind_speed\n",
    "    adjusted_weights = np.clip(weights * adjustments, 0, None)  # Ensure weights are non-negative\n",
    "    return adjusted_weights\n",
    "\n",
    "def normalize_weights(weights):\n",
    "    total_weight = np.sum(weights)\n",
    "    if total_weight == 0:\n",
    "        return np.zeros_like(weights)\n",
    "    return weights / total_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Chailee\\AppData\\Local\\Temp\\ipykernel_18856\\1101129135.py:8: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  wind_data_path = f'{meteostat_folder}\\meteostat{year}.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# interpolate 插值估算\n",
    "def interpolate(data, weights):\n",
    "    return np.sum(weights * data['Value'].values)\n",
    "\n",
    "\n",
    "# 修改 get_wind_data 函数\n",
    "def get_wind_data(date, year):\n",
    "    wind_data_path = f'{meteostat_folder}\\meteostat{year}.csv'\n",
    "    wind_data_df = pd.read_csv(wind_data_path, encoding='utf-8-sig')  # 处理BOM\n",
    "    wind_data_df.columns = wind_data_df.columns.str.strip()\n",
    "\n",
    "    # # Debugging: Print column names to verify\n",
    "    # print(\"Column names in wind_data_df:\", wind_data_df.columns)\n",
    "\n",
    "    # 确保日期格式一致\n",
    "    wind_data_df['date'] = pd.to_datetime(wind_data_df['date'], format='%Y-%m-%d').dt.date\n",
    "    date = pd.to_datetime(date, format='%Y-%m-%d').date()  # 确保date参数也被转换成相同格式\n",
    "\n",
    "    wind_info = wind_data_df[wind_data_df['date'] == date]\n",
    "    if not wind_info.empty:\n",
    "        return wind_info['wspd'].values[0], wind_info['wdir'].values[0]\n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "\n",
    "def interpolate_and_clip(df, column, method='linear', order=2):\n",
    "    # 获取原始列的最小值和最大值\n",
    "    original_min = df[column].min()\n",
    "    original_max = df[column].max()\n",
    "    \n",
    "    # 进行插值\n",
    "    if method == 'polynomial':\n",
    "        interpolated_values = df[column].interpolate(method=method, order=order)\n",
    "    else:\n",
    "        interpolated_values = df[column].interpolate(method=method)\n",
    "    \n",
    "    # 确保插值后的值在原来值的 {min, max} 范围之间\n",
    "    interpolated_values = interpolated_values.clip(lower=original_min, upper=original_max).round(2)\n",
    "    \n",
    "    # 仅更新空值部分\n",
    "    return df[column].combine_first(interpolated_values)\n",
    "\n",
    "# 自定义的多项式插值函数\n",
    "def polynomial_interpolation(series, order=2):\n",
    "    return series.interpolate(method='polynomial', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(base_folder, region):\n",
    "    # 构建文件路径\n",
    "    region_folder = os.path.join(base_folder, region)\n",
    "    no_path = os.path.join(region_folder, f'{region}-Nitric Oxide (ug m-3).csv')\n",
    "    coords_path = os.path.join(base_folder, 'coords_londonair.csv')\n",
    "    \n",
    "    # 读取数据\n",
    "    no_df = pd.read_csv(no_path)\n",
    "    coords_df = pd.read_csv(coords_path)\n",
    "    \n",
    "    # 数据预处理\n",
    "    no_df['ReadingDateTime'] = pd.to_datetime(no_df['ReadingDateTime'], format='%d/%m/%Y %H:%M')\n",
    "    no_df['Date'] = no_df['ReadingDateTime'].dt.date\n",
    "    coords_df[['Latitude', 'Longitude']] = coords_df['Latitude & Longitude'].str.split(', ', expand=True).astype(float)\n",
    "    \n",
    "    # 合并坐标信息\n",
    "    no_df = pd.merge(no_df, coords_df[['Site', 'Latitude', 'Longitude']], on='Site', how='left')\n",
    "    \n",
    "    # 只处理排在最前面的第一种“Site”的Value值\n",
    "    first_site = no_df['Site'].unique()[0]\n",
    "    no_df = no_df[no_df['Site'] == first_site]\n",
    "    \n",
    "    # 检查并处理 NaN 值\n",
    "    if no_df.isnull().values.any():\n",
    "        print(f\"NaN values found in {region} data before interpolation\")\n",
    "    \n",
    "    # Interpolation\n",
    "    no_df['Value'] = no_df['Value'].interpolate(method='time')\n",
    "    \n",
    "    # 对于空缺时间跨度较大的值，根据其他年份同一月份的数据规律进行填充\n",
    "    no_df['Month'] = no_df['ReadingDateTime'].dt.month\n",
    "    monthly_mean = no_df.groupby('Month')['Value'].transform('mean')\n",
    "    no_df['Value'].fillna(monthly_mean, inplace=True)\n",
    "    \n",
    "    # 检查插值结果\n",
    "    if no_df['Value'].isnull().values.any():\n",
    "        print(f\"NaN values found in {region} data after interpolation\")\n",
    "    \n",
    "    return no_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义地区列表\n",
    "regions = [\n",
    "    'Camden', 'City of London', 'Islington', \n",
    "    'Kensington and Chelsea', 'Lambeth', \n",
    "    'Southwark', 'Westminster'\n",
    "]\n",
    "\n",
    "# 定义文件夹路径\n",
    "base_folder = r'D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\AirQuality\\LondonAir'\n",
    "meteostat_folder = r'D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data\\meteostat\\\\'\n",
    "NO_weighted_output_folder = r'D:\\File_auto\\0_UCL_CASA\\OneDrive - University College London\\Xiaoyi_dissertation\\Analysis\\Data_output\\NO_weighted\\\\'\n",
    "\n",
    "# 确保输出文件夹存在\n",
    "if not os.path.exists(NO_weighted_output_folder):\n",
    "    os.makedirs(NO_weighted_output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# 遍历每个地区\n",
    "for region in regions:\n",
    "    print(f\"Processing region: {region}\")\n",
    "    \n",
    "    # 构建文件路径\n",
    "    region_folder = os.path.join(base_folder, region)\n",
    "    no_path = os.path.join(region_folder, f'{region}-Nitric Oxide (ug m-3).csv')\n",
    "    coords_path = os.path.join(base_folder, 'coords_londonair.csv')\n",
    "    \n",
    "    # 读取数据\n",
    "    no_df = pd.read_csv(no_path)\n",
    "    coords_df = pd.read_csv(coords_path)\n",
    "    \n",
    "    # 数据预处理\n",
    "    no_df['ReadingDateTime'] = pd.to_datetime(no_df['ReadingDateTime'], format='%d/%m/%Y %H:%M')\n",
    "    no_df['Date'] = no_df['ReadingDateTime'].dt.date\n",
    "    coords_df[['Latitude', 'Longitude']] = coords_df['Latitude & Longitude'].str.split(', ', expand=True).astype(float)\n",
    "    \n",
    "    # 合并坐标信息\n",
    "    no_df = pd.merge(no_df, coords_df[['Site', 'Latitude', 'Longitude']], on='Site', how='left')\n",
    "    \n",
    "    # Interpolation\n",
    "    no_df['Value'] = interpolate_and_clip(no_df, 'Value', method='linear')\n",
    "    \n",
    "    # Kriging 计算\n",
    "    weighted_values = []\n",
    "    dates = no_df['Date'].unique()\n",
    "    \n",
    "    for date in dates:\n",
    "        daily_data = no_df[no_df['Date'] == date]\n",
    "        if len(daily_data) > 1:\n",
    "            semivariogram = calculate_semivariogram(daily_data)\n",
    "            distances = cdist(daily_data[['Latitude', 'Longitude']], daily_data[['Latitude', 'Longitude']], metric='euclidean')\n",
    "            \n",
    "            # 检查并处理 NaN 值\n",
    "            if np.isnan(distances).any():\n",
    "                distances = np.nan_to_num(distances, nan=0.0)\n",
    "\n",
    "            kriging_weights = calculate_kriging_weights(semivariogram, distances, len(daily_data))\n",
    "            \n",
    "            wind_speed, wind_dir = get_wind_data(date, date.year)\n",
    "            sensor_directions = np.arctan2(daily_data['Longitude'] - daily_data['Longitude'].mean(), daily_data['Latitude'] - daily_data['Latitude'].mean()) * 180 / np.pi\n",
    "            meteostat_path = f'{meteostat_folder}\\\\meteostat{date.year}.csv'\n",
    "            meteostat_df = pd.read_csv(meteostat_path)\n",
    "            max_wind_speed = meteostat_df['wspd'].max()\n",
    "            \n",
    "            adjusted_weights = adjust_weights(kriging_weights, wind_speed, wind_dir, sensor_directions, max_wind_speed)\n",
    "            normalized_weights = normalize_weights(adjusted_weights)\n",
    "            \n",
    "            weighted_value = interpolate(daily_data, normalized_weights)\n",
    "            weighted_value = max(0, weighted_value)  # 确保没有负值\n",
    "            weighted_values.append({'Date': date, 'NO_weighted_value(ug m-3)': weighted_value})\n",
    "        else:\n",
    "            weighted_values.append({'Date': date, 'NO_weighted_value(ug m-3)': daily_data['Value'].values[0]})\n",
    "    # 保存加权后的值\n",
    "    weighted_df = pd.DataFrame(weighted_values)\n",
    "    output_path = os.path.join(NO_weighted_output_folder, f'{region}-NO_weighted.csv')\n",
    "    weighted_df.to_csv(output_path, index=False)\n",
    "    print(f\"Weighted data saved to {output_path}\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing region: Camden\n",
      "NaN values found in Camden data before interpolation\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time-weighted interpolation only works on Series or DataFrames with a DatetimeIndex",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m region \u001b[38;5;129;01min\u001b[39;00m regions:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing region: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     no_df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Kriging 计算\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     weighted_values \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(base_folder, region)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN values found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data before interpolation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Interpolation\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m no_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mno_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mValue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 对于空缺时间跨度较大的值，根据其他年份同一月份的数据规律进行填充\u001b[39;00m\n\u001b[0;32m     31\u001b[0m no_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m no_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReadingDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\pandas\\core\\generic.py:8496\u001b[0m, in \u001b[0;36mNDFrame.interpolate\u001b[1;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[0;32m   8494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   8495\u001b[0m     index \u001b[38;5;241m=\u001b[39m missing\u001b[38;5;241m.\u001b[39mget_interp_index(method, obj\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m-> 8496\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit_area\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_area\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8504\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8505\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   8507\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   8508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_transpose:\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\pandas\\core\\internals\\base.py:291\u001b[0m, in \u001b[0;36mDataManager.interpolate\u001b[1;34m(self, inplace, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minterpolate\u001b[39m(\u001b[38;5;28mself\u001b[39m, inplace: \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minterpolate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1797\u001b[0m, in \u001b[0;36mBlock.interpolate\u001b[1;34m(self, method, index, inplace, limit, limit_direction, limit_area, downcast, using_cow, already_warned, **kwargs)\u001b[0m\n\u001b[0;32m   1794\u001b[0m copy, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_refs_and_copy(using_cow, inplace)\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;66;03m# Dispatch to the EA method.\u001b[39;00m\n\u001b[1;32m-> 1797\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_area\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_area\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1807\u001b[0m data \u001b[38;5;241m=\u001b[39m extract_array(new_values, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1810\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m copy\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m warn_copy_on_write()\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m already_warned \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m already_warned\u001b[38;5;241m.\u001b[39mwarned_already\n\u001b[0;32m   1814\u001b[0m ):\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\pandas\\core\\arrays\\numpy_.py:294\u001b[0m, in \u001b[0;36mNumpyExtensionArray.interpolate\u001b[1;34m(self, method, axis, index, limit, limit_direction, limit_area, copy, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m     out_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ndarray\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# TODO: assert we have floating dtype?\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m \u001b[43mmissing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate_2d_inplace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_area\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_area\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32me:\\Python\\python3.12\\Lib\\site-packages\\pandas\\core\\missing.py:373\u001b[0m, in \u001b[0;36minterpolate_2d_inplace\u001b[1;34m(data, index, axis, method, limit, limit_direction, limit_area, fill_value, mask, **kwargs)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(index\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m--> 373\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    374\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime-weighted interpolation only works \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    375\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon Series or DataFrames with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    376\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatetimeIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m         )\n\u001b[0;32m    378\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    380\u001b[0m limit_direction \u001b[38;5;241m=\u001b[39m validate_limit_direction(limit_direction)\n",
      "\u001b[1;31mValueError\u001b[0m: time-weighted interpolation only works on Series or DataFrames with a DatetimeIndex"
     ]
    }
   ],
   "source": [
    "\n",
    "# 遍历每个地区\n",
    "for region in regions:\n",
    "    print(f\"Processing region: {region}\")\n",
    "    \n",
    "    no_df = preprocess_data(base_folder, region)\n",
    "    \n",
    "    # Kriging 计算\n",
    "    weighted_values = []\n",
    "    dates = no_df['Date'].unique()\n",
    "    \n",
    "    for date in dates:\n",
    "        daily_data = no_df[no_df['Date'] == date]\n",
    "        if len(daily_data) > 1:\n",
    "            semivariogram = calculate_semivariogram(daily_data)\n",
    "            distances = cdist(daily_data[['Latitude', 'Longitude']], daily_data[['Latitude', 'Longitude']], metric='euclidean')\n",
    "            \n",
    "            # 检查并处理 NaN 和 inf 值\n",
    "            if np.isnan(distances).any() or np.isinf(distances).any():\n",
    "                print(f\"NaN or inf values found in distances for {region} on {date}\")\n",
    "                distances = np.nan_to_num(distances, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            kriging_weights = calculate_kriging_weights(semivariogram, distances, len(daily_data))\n",
    "            \n",
    "            wind_speed, wind_dir = get_wind_data(date, date.year)\n",
    "            sensor_directions = np.arctan2(daily_data['Longitude'] - daily_data['Longitude'].mean(), daily_data['Latitude'] - daily_data['Latitude'].mean()) * 180 / np.pi\n",
    "            meteostat_path = f'{meteostat_folder}\\\\meteostat{date.year}.csv'\n",
    "            meteostat_df = pd.read_csv(meteostat_path)\n",
    "            max_wind_speed = meteostat_df['wspd'].max()\n",
    "            \n",
    "            adjusted_weights = adjust_weights(kriging_weights, wind_speed, wind_dir, sensor_directions, max_wind_speed)\n",
    "            normalized_weights = normalize_weights(adjusted_weights)\n",
    "            \n",
    "            weighted_value = interpolate(daily_data, normalized_weights)\n",
    "            weighted_value = max(0, weighted_value)  # 确保没有负值\n",
    "            weighted_values.append({'Date': date, 'NO_weighted_value(ug m-3)': weighted_value})\n",
    "        else:\n",
    "            weighted_values.append({'Date': date, 'NO_weighted_value(ug m-3)': daily_data['Value'].values[0]})\n",
    "    \n",
    "    # 保存加权后的值\n",
    "    weighted_df = pd.DataFrame(weighted_values)\n",
    "    output_path = os.path.join(NO_weighted_output_folder, f'{region}-NO_weighted.csv')\n",
    "    weighted_df.to_csv(output_path, index=False)\n",
    "    print(f\"Weighted data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
